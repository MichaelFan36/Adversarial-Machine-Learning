{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from CWAttack import CW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.utils\n",
    "from torchvision import models\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import torchattacks\n",
    "# from torchattacks import CW\n",
    "\n",
    "\n",
    "mnist_train = dsets.MNIST(root='./data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='./data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# X = pd.read_csv(\"../Research/mnist_train.csv\")\n",
    "# y = pd.read_csv(\"../Research/mnist_test.csv\")\n",
    "\n",
    "digits = load_digits()\n",
    "train_data = digits.data\n",
    "test_data = digits.target\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(digits.data.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "(60000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader2  = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=60000,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader2 = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                         batch_size=10000,\n",
    "                                         shuffle=False)\n",
    "\n",
    "for images, labels in train_loader2:\n",
    "    images_np = images.numpy()\n",
    "    images_re = images_np.reshape(images_np.shape[0],-1)\n",
    "    labels_np = labels.numpy()\n",
    "    print(images_np.shape[0])\n",
    "\n",
    "for images, labels in test_loader2:\n",
    "    images_test_np = images.numpy()\n",
    "    images_test_re = images_test_np.reshape(images_test_np.shape[0],-1)\n",
    "    labels_test_np = labels.numpy()\n",
    "    print(images_re.shape)\n",
    "\n",
    "plt.matshow(images_re[0].reshape(28,28)) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Train_est, Train_val = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "# test_data_train, test_data_test = train_test_split(test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# print(Train_est.shape)\n",
    "# print(test_data_train.shape)\n",
    "\n",
    "# print(Train_val.shape)\n",
    "# print(test_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(images_re, label=labels_np)\n",
    "dtest = xgb.DMatrix(images_test_re, label=labels_test_np)\n",
    "# print(dtest)\n",
    "param = {\n",
    "    'tree_method' : 'gpu_hist',\n",
    "    'max_depth': 5,                 # the maximum depth of each tree\n",
    "    'eta': 0.3,                     # the training step for each iteration\n",
    "    'silent': 1,                    # logging mode - quiet\n",
    "    'objective': 'multi:softmax',   # multiclass classification using the softmax objective\n",
    "    'num_class': 10                 # the number of classes that exist in this datset\n",
    "}  \n",
    "num_round = 500  # the number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:42:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bstmodel = xgb.XGBClassifier(max_depth=5, learning_rate=0.3, n_estimators=500,\n",
    "#                               silent=True, objective='multi:softmax', booster='gbtree')\n",
    "bstmodel2 = xgb.train(param, dtrain, num_round)\n",
    "#Save as human readable model\n",
    "# bstmodel.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# bstmodel.fit(Train_est, test_data_train)\n",
    "# preds = bstmodel.predict(dtest)\n",
    "\n",
    "preds = bstmodel2.predict(dtest)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(labels_test_np, preds)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting tree does not work yet\n",
    "#xgboost.plot_tree(bstmodel, num_trees=2)\n",
    "\n",
    "# xgb.plot_importance(bstmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# clf = xgb.XGBClassifier()\n",
    "# parameters = {\n",
    "#      \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "#      \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "#      \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "#      \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "#      \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "#      }\n",
    "\n",
    "# grid = GridSearchCV(clf,\n",
    "#                     parameters, n_jobs=4,\n",
    "#                     scoring=\"neg_log_loss\",\n",
    "#                     cv=3)\n",
    "\n",
    "\n",
    "# grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_attack(model, images, labels, targeted=False, c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01) :\n",
    "\n",
    "    images = images.to(device)     \n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Define f-function\n",
    "    def f(x) :\n",
    "        print(x.shape)\n",
    "        lists = x[0][0].tolist()\n",
    "        outputs = model.predict(x)\n",
    "        print(outputs.shape)\n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(device)\n",
    "\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "        j = torch.masked_select(outputs, one_hot_labels.byte())\n",
    "        \n",
    "        # If targeted, optimize for making the other class most likely \n",
    "        if targeted :\n",
    "            return torch.clamp(i-j, min=-kappa)\n",
    "        \n",
    "        # If untargeted, optimize for making the other class most likely \n",
    "        else :\n",
    "            return torch.clamp(j-i, min=-kappa)\n",
    "    \n",
    "    w = torch.zeros_like(images, requires_grad=True).to(device)\n",
    "\n",
    "    optimizer = optim.Adam([w], lr=learning_rate)\n",
    "\n",
    "    prev = 1e10\n",
    "    \n",
    "    for step in range(max_iter) :\n",
    "\n",
    "        a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "        loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "        loss2 = torch.sum(c*f(a))\n",
    "\n",
    "        cost = loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Early Stop when loss does not converge.\n",
    "        if step % (max_iter//10) == 0 :\n",
    "            if cost > prev :\n",
    "                print('Attack Stopped due to CONVERGENCE....')\n",
    "                return a\n",
    "            prev = cost\n",
    "        \n",
    "        print('- Learning Progress : %2.2f %%        ' %((step+1)/max_iter*100), end='\\r')\n",
    "\n",
    "    attack_images = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hummingbird.ml import convert\n",
    "\n",
    "bstmodel_convert = convert(bstmodel, 'pytorch')\n",
    "# bstmodel_convert.cuda()\n",
    "# bstmodel_convert.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(images[0][0].shape)\n",
    "    images = cw_attack(bstmodel_convert, images, labels, targeted=False, c=0.1)\n",
    "    outputs = bstmodel_convert(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += 1\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
