{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import CW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Creating dataset using torch dataloaders\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))]) # Normalizing dataset\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# Initialize GPU\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model structure\n",
    "class HNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(HNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubModel Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submodel Structure for training residual\n",
    "class NHNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NHNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some data structures to store useful data\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial model\n",
    "initial_model = HNet()\n",
    "\n",
    "#Create the optimizer for the initial model\n",
    "optimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "# Create Loss function for the intial model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#Change model into cuda mode\n",
    "if torch.cuda.is_available():\n",
    "    initial_model = initial_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function for Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model Training Function\n",
    "def train(epoch):\n",
    "    initial_model.train()\n",
    "#     exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = initial_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        # torch.save(initial_model.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model.pth')\n",
    "        # torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/optimizer.pth')\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function for Intial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Model Evaluating Function\n",
    "def evaluate(data_loader):\n",
    "    initial_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = initial_model(data)\n",
    "        \n",
    "            loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    test_losses.append(loss)    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training the Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.661364\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.601474\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.621534\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.655728\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.572359\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.599732\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.547913\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.559445\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.511098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cozyn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 1.5579, Accuracy: 54310/60000 (90.517%)\n",
      "\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.528747\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.615363\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.541097\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.558620\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.527702\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.518819\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.530707\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.560704\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.515562\n",
      "\n",
      "Average loss: 1.5430, Accuracy: 55119/60000 (91.865%)\n",
      "\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.537167\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.549802\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.491152\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.537123\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.493295\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.560355\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.603831\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.559644\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.573228\n",
      "\n",
      "Average loss: 1.5471, Accuracy: 54822/60000 (91.370%)\n",
      "\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.523593\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.512303\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.538611\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.495459\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.553376\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.549120\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.586058\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.478601\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.551736\n",
      "\n",
      "Average loss: 1.5366, Accuracy: 55458/60000 (92.430%)\n",
      "\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.493742\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.539187\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.525622\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.523596\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.508068\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.507878\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.534779\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.556631\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.492405\n",
      "\n",
      "Average loss: 1.5412, Accuracy: 55166/60000 (91.943%)\n",
      "\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.567551\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.523561\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.485108\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.542923\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.562421\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.556254\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.554408\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.570526\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.592594\n",
      "\n",
      "Average loss: 1.5234, Accuracy: 56257/60000 (93.762%)\n",
      "\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.516947\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.539363\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.554840\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.539360\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.594142\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.551241\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.538973\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.569843\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.523193\n",
      "\n",
      "Average loss: 1.5394, Accuracy: 55291/60000 (92.152%)\n",
      "\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.492329\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.539100\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.539584\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.539924\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.539671\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.507979\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.633107\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.492429\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.524020\n",
      "\n",
      "Average loss: 1.5335, Accuracy: 55638/60000 (92.730%)\n",
      "\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.476907\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.554901\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.523652\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.547815\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.516273\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.510179\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.476205\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.510963\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.522561\n",
      "\n",
      "Average loss: 1.5343, Accuracy: 55603/60000 (92.672%)\n",
      "\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.525871\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.476776\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.541446\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.519721\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.557306\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.492299\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.508096\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.569704\n",
      "\n",
      "Average loss: 1.5493, Accuracy: 54696/60000 (91.160%)\n",
      "\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.492465\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.540438\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.542354\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.523639\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.477818\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.556064\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.492401\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.461151\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.523669\n",
      "\n",
      "Average loss: 1.5294, Accuracy: 55892/60000 (93.153%)\n",
      "\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.554882\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.539287\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 1.539276\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.523046\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 1.570472\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.561619\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.492761\n",
      "\n",
      "Average loss: 1.5154, Accuracy: 56729/60000 (94.548%)\n",
      "\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.544811\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 1.566981\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.523255\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 1.601206\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.507871\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 1.523651\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.585417\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 1.523635\n",
      "\n",
      "Average loss: 1.5350, Accuracy: 55551/60000 (92.585%)\n",
      "\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 1.492401\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.605629\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 1.555600\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.471212\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.538604\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 1.541631\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.508030\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 1.492346\n",
      "\n",
      "Average loss: 1.5280, Accuracy: 55982/60000 (93.303%)\n",
      "\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 1.586144\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.586371\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.574805\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 1.476776\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 1.617341\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.570278\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 1.552290\n",
      "\n",
      "Average loss: 1.5331, Accuracy: 55677/60000 (92.795%)\n",
      "\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 1.539551\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 1.537461\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 1.554914\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.523651\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 1.560154\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.508032\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 1.523861\n",
      "\n",
      "Average loss: 1.5333, Accuracy: 55666/60000 (92.777%)\n",
      "\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 1.508026\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.507984\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 1.462147\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.523652\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 1.523651\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.587738\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 1.507848\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.461151\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 1.508120\n",
      "\n",
      "Average loss: 1.5642, Accuracy: 53807/60000 (89.678%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 1.555029\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.539479\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 1.539276\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.523724\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 1.492603\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.543978\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 1.508083\n",
      "\n",
      "Average loss: 1.5426, Accuracy: 55116/60000 (91.860%)\n",
      "\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.511070\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 1.476776\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 1.585746\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 1.554097\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.519577\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 1.569870\n",
      "\n",
      "Average loss: 1.5138, Accuracy: 56836/60000 (94.727%)\n",
      "\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 1.586151\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.585894\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 1.506047\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.607012\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 1.537608\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 1.523072\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 1.586150\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.570620\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 1.523653\n",
      "\n",
      "Average loss: 1.5203, Accuracy: 56450/60000 (94.083%)\n",
      "\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 1.507984\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 1.476778\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.492430\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 1.523653\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 1.523514\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.530132\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5179, Accuracy: 56593/60000 (94.322%)\n",
      "\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 1.461151\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 1.570530\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 1.570484\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 1.526224\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 1.585052\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.554622\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 1.492403\n",
      "\n",
      "Average loss: 1.5379, Accuracy: 55397/60000 (92.328%)\n",
      "\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 1.553910\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 1.554310\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 1.539277\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 1.523659\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 1.569955\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 1.554868\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 1.523650\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 1.583536\n",
      "\n",
      "Average loss: 1.5364, Accuracy: 55479/60000 (92.465%)\n",
      "\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 1.574254\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 1.492401\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.539276\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 1.586151\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 1.476776\n",
      "\n",
      "Average loss: 1.5267, Accuracy: 56065/60000 (93.442%)\n",
      "\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 1.570526\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 1.554898\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 1.523656\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 1.615522\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 1.492401\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 1.539304\n",
      "\n",
      "Average loss: 1.5422, Accuracy: 55134/60000 (91.890%)\n",
      "\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 1.508044\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 1.507684\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 1.523651\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 1.539278\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 1.562671\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 1.508110\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 1.493862\n",
      "\n",
      "Average loss: 1.5249, Accuracy: 56174/60000 (93.623%)\n",
      "\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 1.509135\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.553833\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 1.461151\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 1.529600\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 1.586193\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.570526\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5191, Accuracy: 56525/60000 (94.208%)\n",
      "\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 1.586151\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 1.523651\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.539262\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 1.508018\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 1.554901\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.617568\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 1.616813\n",
      "\n",
      "Average loss: 1.5225, Accuracy: 56311/60000 (93.852%)\n",
      "\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 1.508025\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 1.554901\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 1.523641\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 1.509010\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 1.570526\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 1.539276\n",
      "\n",
      "Average loss: 1.5501, Accuracy: 54655/60000 (91.092%)\n",
      "\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 1.574983\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 1.539349\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 1.539276\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 1.554901\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 1.586151\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 1.506154\n",
      "\n",
      "Average loss: 1.5578, Accuracy: 54199/60000 (90.332%)\n",
      "\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 1.539276\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 1.540222\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 1.521152\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 1.523651\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 1.461151\n",
      "\n",
      "Average loss: 1.5240, Accuracy: 56224/60000 (93.707%)\n",
      "\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 1.492401\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 1.601776\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 1.523620\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 1.495658\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 1.523651\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 1.539423\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 1.523207\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 1.559003\n",
      "\n",
      "Average loss: 1.5223, Accuracy: 56327/60000 (93.878%)\n",
      "\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 1.508026\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 1.570526\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 1.492401\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.570513\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 1.523653\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 1.492401\n",
      "\n",
      "Average loss: 1.5236, Accuracy: 56248/60000 (93.747%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 1.492422\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 1.617339\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 1.539276\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 1.523653\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 1.586114\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5344, Accuracy: 55598/60000 (92.663%)\n",
      "\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 1.570526\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 1.492401\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 1.492404\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 1.586151\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 1.586151\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 1.648649\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 1.601776\n",
      "\n",
      "Average loss: 1.5411, Accuracy: 55202/60000 (92.003%)\n",
      "\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 1.531138\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 1.538849\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 1.522290\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 1.570592\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 1.601768\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 1.538896\n",
      "\n",
      "Average loss: 1.5369, Accuracy: 55450/60000 (92.417%)\n",
      "\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 1.570513\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 1.554901\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 1.554901\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 1.601822\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 1.571333\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 1.492401\n",
      "\n",
      "Average loss: 1.5275, Accuracy: 56018/60000 (93.363%)\n",
      "\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 1.492401\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 1.492392\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 1.508039\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 1.491838\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 1.535220\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5442, Accuracy: 55011/60000 (91.685%)\n",
      "\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 1.508026\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 1.523651\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 1.523603\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 1.572619\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5257, Accuracy: 56127/60000 (93.545%)\n",
      "\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 1.679901\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 1.491471\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 1.561290\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 1.554901\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 1.492401\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 1.586151\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 1.523453\n",
      "\n",
      "Average loss: 1.5330, Accuracy: 55689/60000 (92.815%)\n",
      "\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 1.492401\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 1.461206\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 1.523651\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 1.554908\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5430, Accuracy: 55085/60000 (91.808%)\n",
      "\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 1.492421\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 1.523651\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 1.554846\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 1.535935\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 1.492401\n",
      "\n",
      "Average loss: 1.5305, Accuracy: 55834/60000 (93.057%)\n",
      "\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 1.601776\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 1.617401\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 1.586150\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 1.539255\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 1.476776\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 1.570526\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 1.586149\n",
      "\n",
      "Average loss: 1.5380, Accuracy: 55385/60000 (92.308%)\n",
      "\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 1.696834\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 1.688166\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 1.601776\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 1.523651\n",
      "\n",
      "Average loss: 1.5376, Accuracy: 55412/60000 (92.353%)\n",
      "\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 1.554790\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 1.570526\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 1.586151\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 1.570526\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 1.554901\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 1.633026\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5862, Accuracy: 52494/60000 (87.490%)\n",
      "\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 1.570526\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 1.554911\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 1.492401\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 1.586151\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 1.554901\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 1.511940\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 1.570369\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5299, Accuracy: 55874/60000 (93.123%)\n",
      "\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 1.509202\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 1.492401\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 1.539277\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 1.570526\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 1.551405\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 1.633026\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 1.570706\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 1.648651\n",
      "\n",
      "Average loss: 1.5552, Accuracy: 54355/60000 (90.592%)\n",
      "\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 1.554960\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 1.586151\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 1.632983\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 1.601776\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 1.539280\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 1.492401\n",
      "\n",
      "Average loss: 1.5513, Accuracy: 54592/60000 (90.987%)\n",
      "\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 1.492401\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 1.586151\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 1.570526\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 1.476776\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 1.539276\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 1.601776\n",
      "\n",
      "Average loss: 1.5333, Accuracy: 55671/60000 (92.785%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 1.547493\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 1.554901\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 1.570526\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 1.570526\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 1.570526\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 1.585912\n",
      "\n",
      "Average loss: 1.5399, Accuracy: 55272/60000 (92.120%)\n",
      "\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 1.633026\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 1.601776\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 1.523676\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 1.539300\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5338, Accuracy: 55642/60000 (92.737%)\n",
      "\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 1.492401\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 1.586151\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 1.539275\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 1.461377\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 1.586151\n",
      "\n",
      "Average loss: 1.5488, Accuracy: 54742/60000 (91.237%)\n",
      "\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 1.539276\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 1.619408\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 1.554901\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 1.554901\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 1.525098\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5420, Accuracy: 55142/60000 (91.903%)\n",
      "\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 1.523651\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 1.539335\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 1.539276\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 1.554900\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 1.570526\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 1.492401\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 1.601776\n",
      "\n",
      "Average loss: 1.5179, Accuracy: 56596/60000 (94.327%)\n",
      "\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 1.570526\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 1.525370\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 1.523651\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 1.554901\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 1.617401\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 1.539276\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 1.508625\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 1.537339\n",
      "\n",
      "Average loss: 1.5501, Accuracy: 54660/60000 (91.100%)\n",
      "\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 1.554901\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 1.570529\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 1.507869\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 1.508026\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 1.523651\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 1.554901\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 1.539276\n",
      "\n",
      "Average loss: 1.5531, Accuracy: 54481/60000 (90.802%)\n",
      "\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 1.679856\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 1.586151\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 1.539276\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 1.539276\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 1.617401\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 1.570484\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 1.539245\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 1.556363\n",
      "\n",
      "Average loss: 1.5728, Accuracy: 53299/60000 (88.832%)\n",
      "\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 1.601776\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 1.578069\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 1.617401\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 1.539276\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 1.492401\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 1.574998\n",
      "\n",
      "Average loss: 1.5741, Accuracy: 53222/60000 (88.703%)\n",
      "\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 1.601778\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 1.586151\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 1.586151\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 1.554901\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 1.586151\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 1.633026\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 1.586151\n",
      "\n",
      "Average loss: 1.5594, Accuracy: 54105/60000 (90.175%)\n",
      "\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 1.554901\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 1.617401\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 1.554901\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 1.570526\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 1.554900\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 1.586151\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 1.523651\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 1.590995\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5780, Accuracy: 52985/60000 (88.308%)\n",
      "\n",
      "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 1.617405\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 1.570526\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 1.570526\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 1.554901\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 1.601776\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 1.492401\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 1.601775\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 1.523651\n",
      "\n",
      "Average loss: 1.5361, Accuracy: 55498/60000 (92.497%)\n",
      "\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 1.601776\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 1.508026\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 1.539276\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 1.523651\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 1.554901\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 1.492401\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 1.523651\n",
      "\n",
      "Average loss: 1.5299, Accuracy: 55875/60000 (93.125%)\n",
      "\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 1.554901\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 1.601776\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 1.601776\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 1.633026\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 1.507959\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 1.508026\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 1.617400\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 1.648651\n",
      "\n",
      "Average loss: 1.5576, Accuracy: 54211/60000 (90.352%)\n",
      "\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 1.539276\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 1.570526\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 1.523651\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 1.554901\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 1.554901\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 1.570526\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 1.523611\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 1.554901\n",
      "\n",
      "Average loss: 1.5316, Accuracy: 55772/60000 (92.953%)\n",
      "\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 1.492401\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 1.542543\n",
      "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 1.536450\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 1.601776\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 1.492401\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 1.508026\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 1.539247\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 1.554901\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 1.601776\n",
      "\n",
      "Average loss: 1.5549, Accuracy: 54377/60000 (90.628%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "n_epochs = 65\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Load the Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this model so that I dont have to train again in the future\n",
    "torch.save(initial_model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HNet(\n",
       "  (flatten): Flatten()\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from local file\n",
    "initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculatee mse residual and normalize the result to values between 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate mse residual based on the wiki\n",
    "def mseresidual(y, F):\n",
    "    residual = y - F\n",
    "    absolute = torch.abs(residual)\n",
    "    residual = residual / torch.max(absolute)\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Code for gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function used to train and find optimal gamma for submodels\n",
    "#input: intial model that's already trained\n",
    "#M is number of submodels needed to be trained\n",
    "def GradientBoosting(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64) # used to hold the final optimized gamma\n",
    "    models = [] # used to hold all the models\n",
    "    residual_list = [] # used to hold the residual of each batch calculated\n",
    "    for m in range(M):\n",
    "        # Intialize submodels\n",
    "        Hmodel = NHNet()\n",
    "        Hcriterion = nn.MSELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "            Hcriterion = Hcriterion.cuda()\n",
    "            \n",
    "        # Start Training\n",
    "        epoch = 5\n",
    "        Hmodel.train()\n",
    "        Hoptimizer = optim.Adam(Hmodel.parameters(), lr=0.001)\n",
    "        for i in range(epoch):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                # Create one-hot label target tensor\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                # Calculate F(x)\n",
    "                Hoptimizer.zero_grad()\n",
    "                output = initial_model(data)\n",
    "                # Calculate the output from all the models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        output = output.cuda()\n",
    "                        model = model.cuda()\n",
    "                    output = output + gamma_exp[j] * model(data)\n",
    "#                 print(\"output is:\", output)\n",
    "                #Convert into Onehot label so that it would be able to calculate the residual\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                #Calculate Residual\n",
    "#                 print(\"target_onehot is:\", target_onehot)\n",
    "#                 print(\"output is:\", output)\n",
    "                residual = mseresidual(target_onehot, output)\n",
    "                houtput = Hmodel(data)\n",
    "#                 print(\"houtput is:\", houtput)\n",
    "                residual = residual + 1e-6\n",
    "#                 print(residual)\n",
    "                residual = residual.type(torch.cuda.FloatTensor)\n",
    "                houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                #Calculate the loss\n",
    "                loss = Hcriterion(houtput, residual)\n",
    "                loss.backward(retain_graph = True)\n",
    "                Hoptimizer.step()\n",
    "                # Print out current Process\n",
    "#                 if (batch_idx + 1)% 100 == 0 and i % 10 == 0:\n",
    "                print('Train Epoch: Model Number: {} {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        m+1,i, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                        100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "        models.append(Hmodel)\n",
    "        \n",
    "        # Initialize a random gamma\n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.01)\n",
    "        Gcriterion = nn.MSELoss()\n",
    "        # Start finding the best gamma\n",
    "        for i in range(20):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                    \n",
    "                #Calculate the initial output\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                #Calculate the final output by combining all previous models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[j]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "                # Covert into one-hot label\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                # Get the currect model output\n",
    "                temp = Hmodel(data)\n",
    "                # Find the current ensemble model output\n",
    "                predicted = output + gamma * temp\n",
    "                # Calculate the loss\n",
    "                loss = Gcriterion(predicted, target_onehot)\n",
    "                loss.backward(retain_graph = True)\n",
    "                # Optimize the gamma\n",
    "                Goptimizer.step()  \n",
    "                \n",
    "        gamma_exp[m] = gamma\n",
    "    print(gamma_exp)\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Gradient Boosting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [64/60000 (0%)]\tLoss: 0.031816\n",
      "Train Epoch: Model Number: 1 0 [128/60000 (0%)]\tLoss: 0.028726\n",
      "Train Epoch: Model Number: 1 0 [192/60000 (0%)]\tLoss: 0.022739\n",
      "Train Epoch: Model Number: 1 0 [256/60000 (0%)]\tLoss: 0.028536\n",
      "Train Epoch: Model Number: 1 0 [320/60000 (1%)]\tLoss: 0.016434\n",
      "Train Epoch: Model Number: 1 0 [384/60000 (1%)]\tLoss: 0.034890\n",
      "Train Epoch: Model Number: 1 0 [448/60000 (1%)]\tLoss: 0.025607\n",
      "Train Epoch: Model Number: 1 0 [512/60000 (1%)]\tLoss: 0.034490\n",
      "Train Epoch: Model Number: 1 0 [576/60000 (1%)]\tLoss: 0.028725\n",
      "Train Epoch: Model Number: 1 0 [640/60000 (1%)]\tLoss: 0.025296\n",
      "Train Epoch: Model Number: 1 0 [704/60000 (1%)]\tLoss: 0.036660\n",
      "Train Epoch: Model Number: 1 0 [768/60000 (1%)]\tLoss: 0.021958\n",
      "Train Epoch: Model Number: 1 0 [832/60000 (1%)]\tLoss: 0.031298\n",
      "Train Epoch: Model Number: 1 0 [896/60000 (1%)]\tLoss: 0.025380\n",
      "Train Epoch: Model Number: 1 0 [960/60000 (2%)]\tLoss: 0.019574\n",
      "Train Epoch: Model Number: 1 0 [1024/60000 (2%)]\tLoss: 0.040457\n",
      "Train Epoch: Model Number: 1 0 [1088/60000 (2%)]\tLoss: 0.031510\n",
      "Train Epoch: Model Number: 1 0 [1152/60000 (2%)]\tLoss: 0.028362\n",
      "Train Epoch: Model Number: 1 0 [1216/60000 (2%)]\tLoss: 0.016220\n",
      "Train Epoch: Model Number: 1 0 [1280/60000 (2%)]\tLoss: 0.025537\n",
      "Train Epoch: Model Number: 1 0 [1344/60000 (2%)]\tLoss: 0.034128\n",
      "Train Epoch: Model Number: 1 0 [1408/60000 (2%)]\tLoss: 0.028214\n",
      "Train Epoch: Model Number: 1 0 [1472/60000 (2%)]\tLoss: 0.021983\n",
      "Train Epoch: Model Number: 1 0 [1536/60000 (3%)]\tLoss: 0.031899\n",
      "Train Epoch: Model Number: 1 0 [1600/60000 (3%)]\tLoss: 0.027975\n",
      "Train Epoch: Model Number: 1 0 [1664/60000 (3%)]\tLoss: 0.028614\n",
      "Train Epoch: Model Number: 1 0 [1728/60000 (3%)]\tLoss: 0.025329\n",
      "Train Epoch: Model Number: 1 0 [1792/60000 (3%)]\tLoss: 0.028286\n",
      "Train Epoch: Model Number: 1 0 [1856/60000 (3%)]\tLoss: 0.030756\n",
      "Train Epoch: Model Number: 1 0 [1920/60000 (3%)]\tLoss: 0.037260\n",
      "Train Epoch: Model Number: 1 0 [1984/60000 (3%)]\tLoss: 0.027388\n",
      "Train Epoch: Model Number: 1 0 [2048/60000 (3%)]\tLoss: 0.033644\n",
      "Train Epoch: Model Number: 1 0 [2112/60000 (4%)]\tLoss: 0.025103\n",
      "Train Epoch: Model Number: 1 0 [2176/60000 (4%)]\tLoss: 0.027626\n",
      "Train Epoch: Model Number: 1 0 [2240/60000 (4%)]\tLoss: 0.016877\n",
      "Train Epoch: Model Number: 1 0 [2304/60000 (4%)]\tLoss: 0.031586\n",
      "Train Epoch: Model Number: 1 0 [2368/60000 (4%)]\tLoss: 0.031618\n",
      "Train Epoch: Model Number: 1 0 [2432/60000 (4%)]\tLoss: 0.028353\n",
      "Train Epoch: Model Number: 1 0 [2496/60000 (4%)]\tLoss: 0.031542\n",
      "Train Epoch: Model Number: 1 0 [2560/60000 (4%)]\tLoss: 0.028940\n",
      "Train Epoch: Model Number: 1 0 [2624/60000 (4%)]\tLoss: 0.037373\n",
      "Train Epoch: Model Number: 1 0 [2688/60000 (4%)]\tLoss: 0.032683\n",
      "Train Epoch: Model Number: 1 0 [2752/60000 (5%)]\tLoss: 0.028547\n",
      "Train Epoch: Model Number: 1 0 [2816/60000 (5%)]\tLoss: 0.028179\n",
      "Train Epoch: Model Number: 1 0 [2880/60000 (5%)]\tLoss: 0.031822\n",
      "Train Epoch: Model Number: 1 0 [2944/60000 (5%)]\tLoss: 0.025837\n",
      "Train Epoch: Model Number: 1 0 [3008/60000 (5%)]\tLoss: 0.025342\n",
      "Train Epoch: Model Number: 1 0 [3072/60000 (5%)]\tLoss: 0.017085\n",
      "Train Epoch: Model Number: 1 0 [3136/60000 (5%)]\tLoss: 0.021608\n",
      "Train Epoch: Model Number: 1 0 [3200/60000 (5%)]\tLoss: 0.027097\n",
      "Train Epoch: Model Number: 1 0 [3264/60000 (5%)]\tLoss: 0.037470\n",
      "Train Epoch: Model Number: 1 0 [3328/60000 (6%)]\tLoss: 0.028145\n",
      "Train Epoch: Model Number: 1 0 [3392/60000 (6%)]\tLoss: 0.028505\n",
      "Train Epoch: Model Number: 1 0 [3456/60000 (6%)]\tLoss: 0.019693\n",
      "Train Epoch: Model Number: 1 0 [3520/60000 (6%)]\tLoss: 0.040311\n",
      "Train Epoch: Model Number: 1 0 [3584/60000 (6%)]\tLoss: 0.022676\n",
      "Train Epoch: Model Number: 1 0 [3648/60000 (6%)]\tLoss: 0.025366\n",
      "Train Epoch: Model Number: 1 0 [3712/60000 (6%)]\tLoss: 0.025610\n",
      "Train Epoch: Model Number: 1 0 [3776/60000 (6%)]\tLoss: 0.024625\n",
      "Train Epoch: Model Number: 1 0 [3840/60000 (6%)]\tLoss: 0.037688\n",
      "Train Epoch: Model Number: 1 0 [3904/60000 (7%)]\tLoss: 0.033287\n",
      "Train Epoch: Model Number: 1 0 [3968/60000 (7%)]\tLoss: 0.026131\n",
      "Train Epoch: Model Number: 1 0 [4032/60000 (7%)]\tLoss: 0.031038\n",
      "Train Epoch: Model Number: 1 0 [4096/60000 (7%)]\tLoss: 0.022592\n",
      "Train Epoch: Model Number: 1 0 [4160/60000 (7%)]\tLoss: 0.017104\n",
      "Train Epoch: Model Number: 1 0 [4224/60000 (7%)]\tLoss: 0.046189\n",
      "Train Epoch: Model Number: 1 0 [4288/60000 (7%)]\tLoss: 0.027155\n",
      "Train Epoch: Model Number: 1 0 [4352/60000 (7%)]\tLoss: 0.031985\n",
      "Train Epoch: Model Number: 1 0 [4416/60000 (7%)]\tLoss: 0.019412\n",
      "Train Epoch: Model Number: 1 0 [4480/60000 (7%)]\tLoss: 0.019644\n",
      "Train Epoch: Model Number: 1 0 [4544/60000 (8%)]\tLoss: 0.037105\n",
      "Train Epoch: Model Number: 1 0 [4608/60000 (8%)]\tLoss: 0.022292\n",
      "Train Epoch: Model Number: 1 0 [4672/60000 (8%)]\tLoss: 0.019138\n",
      "Train Epoch: Model Number: 1 0 [4736/60000 (8%)]\tLoss: 0.045831\n",
      "Train Epoch: Model Number: 1 0 [4800/60000 (8%)]\tLoss: 0.019403\n",
      "Train Epoch: Model Number: 1 0 [4864/60000 (8%)]\tLoss: 0.028561\n",
      "Train Epoch: Model Number: 1 0 [4928/60000 (8%)]\tLoss: 0.022806\n",
      "Train Epoch: Model Number: 1 0 [4992/60000 (8%)]\tLoss: 0.028705\n",
      "Train Epoch: Model Number: 1 0 [5056/60000 (8%)]\tLoss: 0.028274\n",
      "Train Epoch: Model Number: 1 0 [5120/60000 (9%)]\tLoss: 0.039730\n",
      "Train Epoch: Model Number: 1 0 [5184/60000 (9%)]\tLoss: 0.040343\n",
      "Train Epoch: Model Number: 1 0 [5248/60000 (9%)]\tLoss: 0.027934\n",
      "Train Epoch: Model Number: 1 0 [5312/60000 (9%)]\tLoss: 0.030568\n",
      "Train Epoch: Model Number: 1 0 [5376/60000 (9%)]\tLoss: 0.030399\n",
      "Train Epoch: Model Number: 1 0 [5440/60000 (9%)]\tLoss: 0.027948\n",
      "Train Epoch: Model Number: 1 0 [5504/60000 (9%)]\tLoss: 0.025425\n",
      "Train Epoch: Model Number: 1 0 [5568/60000 (9%)]\tLoss: 0.031233\n",
      "Train Epoch: Model Number: 1 0 [5632/60000 (9%)]\tLoss: 0.035697\n",
      "Train Epoch: Model Number: 1 0 [5696/60000 (9%)]\tLoss: 0.016700\n",
      "Train Epoch: Model Number: 1 0 [5760/60000 (10%)]\tLoss: 0.028031\n",
      "Train Epoch: Model Number: 1 0 [5824/60000 (10%)]\tLoss: 0.022791\n",
      "Train Epoch: Model Number: 1 0 [5888/60000 (10%)]\tLoss: 0.016526\n",
      "Train Epoch: Model Number: 1 0 [5952/60000 (10%)]\tLoss: 0.031630\n",
      "Train Epoch: Model Number: 1 0 [6016/60000 (10%)]\tLoss: 0.019198\n",
      "Train Epoch: Model Number: 1 0 [6080/60000 (10%)]\tLoss: 0.031420\n",
      "Train Epoch: Model Number: 1 0 [6144/60000 (10%)]\tLoss: 0.024979\n",
      "Train Epoch: Model Number: 1 0 [6208/60000 (10%)]\tLoss: 0.021865\n",
      "Train Epoch: Model Number: 1 0 [6272/60000 (10%)]\tLoss: 0.025580\n",
      "Train Epoch: Model Number: 1 0 [6336/60000 (11%)]\tLoss: 0.031536\n",
      "Train Epoch: Model Number: 1 0 [6400/60000 (11%)]\tLoss: 0.019698\n",
      "Train Epoch: Model Number: 1 0 [6464/60000 (11%)]\tLoss: 0.027528\n",
      "Train Epoch: Model Number: 1 0 [6528/60000 (11%)]\tLoss: 0.037295\n",
      "Train Epoch: Model Number: 1 0 [6592/60000 (11%)]\tLoss: 0.025445\n",
      "Train Epoch: Model Number: 1 0 [6656/60000 (11%)]\tLoss: 0.015984\n",
      "Train Epoch: Model Number: 1 0 [6720/60000 (11%)]\tLoss: 0.019559\n",
      "Train Epoch: Model Number: 1 0 [6784/60000 (11%)]\tLoss: 0.033921\n",
      "Train Epoch: Model Number: 1 0 [6848/60000 (11%)]\tLoss: 0.040555\n",
      "Train Epoch: Model Number: 1 0 [6912/60000 (12%)]\tLoss: 0.022725\n",
      "Train Epoch: Model Number: 1 0 [6976/60000 (12%)]\tLoss: 0.031515\n",
      "Train Epoch: Model Number: 1 0 [7040/60000 (12%)]\tLoss: 0.031182\n",
      "Train Epoch: Model Number: 1 0 [7104/60000 (12%)]\tLoss: 0.027816\n",
      "Train Epoch: Model Number: 1 0 [7168/60000 (12%)]\tLoss: 0.019773\n",
      "Train Epoch: Model Number: 1 0 [7232/60000 (12%)]\tLoss: 0.018981\n",
      "Train Epoch: Model Number: 1 0 [7296/60000 (12%)]\tLoss: 0.013359\n",
      "Train Epoch: Model Number: 1 0 [7360/60000 (12%)]\tLoss: 0.035408\n",
      "Train Epoch: Model Number: 1 0 [7424/60000 (12%)]\tLoss: 0.024406\n",
      "Train Epoch: Model Number: 1 0 [7488/60000 (12%)]\tLoss: 0.028250\n",
      "Train Epoch: Model Number: 1 0 [7552/60000 (13%)]\tLoss: 0.027796\n",
      "Train Epoch: Model Number: 1 0 [7616/60000 (13%)]\tLoss: 0.024536\n",
      "Train Epoch: Model Number: 1 0 [7680/60000 (13%)]\tLoss: 0.022203\n",
      "Train Epoch: Model Number: 1 0 [7744/60000 (13%)]\tLoss: 0.025314\n",
      "Train Epoch: Model Number: 1 0 [7808/60000 (13%)]\tLoss: 0.016787\n",
      "Train Epoch: Model Number: 1 0 [7872/60000 (13%)]\tLoss: 0.026418\n",
      "Train Epoch: Model Number: 1 0 [7936/60000 (13%)]\tLoss: 0.013637\n",
      "Train Epoch: Model Number: 1 0 [8000/60000 (13%)]\tLoss: 0.027377\n",
      "Train Epoch: Model Number: 1 0 [8064/60000 (13%)]\tLoss: 0.030587\n",
      "Train Epoch: Model Number: 1 0 [8128/60000 (14%)]\tLoss: 0.028048\n",
      "Train Epoch: Model Number: 1 0 [8192/60000 (14%)]\tLoss: 0.036690\n",
      "Train Epoch: Model Number: 1 0 [8256/60000 (14%)]\tLoss: 0.028451\n",
      "Train Epoch: Model Number: 1 0 [8320/60000 (14%)]\tLoss: 0.029964\n",
      "Train Epoch: Model Number: 1 0 [8384/60000 (14%)]\tLoss: 0.028820\n",
      "Train Epoch: Model Number: 1 0 [8448/60000 (14%)]\tLoss: 0.033765\n",
      "Train Epoch: Model Number: 1 0 [8512/60000 (14%)]\tLoss: 0.018718\n",
      "Train Epoch: Model Number: 1 0 [8576/60000 (14%)]\tLoss: 0.018396\n",
      "Train Epoch: Model Number: 1 0 [8640/60000 (14%)]\tLoss: 0.026905\n",
      "Train Epoch: Model Number: 1 0 [8704/60000 (14%)]\tLoss: 0.031950\n",
      "Train Epoch: Model Number: 1 0 [8768/60000 (15%)]\tLoss: 0.025386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [8832/60000 (15%)]\tLoss: 0.022255\n",
      "Train Epoch: Model Number: 1 0 [8896/60000 (15%)]\tLoss: 0.022428\n",
      "Train Epoch: Model Number: 1 0 [8960/60000 (15%)]\tLoss: 0.036384\n",
      "Train Epoch: Model Number: 1 0 [9024/60000 (15%)]\tLoss: 0.025807\n",
      "Train Epoch: Model Number: 1 0 [9088/60000 (15%)]\tLoss: 0.031026\n",
      "Train Epoch: Model Number: 1 0 [9152/60000 (15%)]\tLoss: 0.019149\n",
      "Train Epoch: Model Number: 1 0 [9216/60000 (15%)]\tLoss: 0.042706\n",
      "Train Epoch: Model Number: 1 0 [9280/60000 (15%)]\tLoss: 0.021859\n",
      "Train Epoch: Model Number: 1 0 [9344/60000 (16%)]\tLoss: 0.031711\n",
      "Train Epoch: Model Number: 1 0 [9408/60000 (16%)]\tLoss: 0.031255\n",
      "Train Epoch: Model Number: 1 0 [9472/60000 (16%)]\tLoss: 0.016654\n",
      "Train Epoch: Model Number: 1 0 [9536/60000 (16%)]\tLoss: 0.030371\n",
      "Train Epoch: Model Number: 1 0 [9600/60000 (16%)]\tLoss: 0.016581\n",
      "Train Epoch: Model Number: 1 0 [9664/60000 (16%)]\tLoss: 0.039357\n",
      "Train Epoch: Model Number: 1 0 [9728/60000 (16%)]\tLoss: 0.025016\n",
      "Train Epoch: Model Number: 1 0 [9792/60000 (16%)]\tLoss: 0.027919\n",
      "Train Epoch: Model Number: 1 0 [9856/60000 (16%)]\tLoss: 0.024436\n",
      "Train Epoch: Model Number: 1 0 [9920/60000 (17%)]\tLoss: 0.018135\n",
      "Train Epoch: Model Number: 1 0 [9984/60000 (17%)]\tLoss: 0.029041\n",
      "Train Epoch: Model Number: 1 0 [10048/60000 (17%)]\tLoss: 0.027459\n",
      "Train Epoch: Model Number: 1 0 [10112/60000 (17%)]\tLoss: 0.022098\n",
      "Train Epoch: Model Number: 1 0 [10176/60000 (17%)]\tLoss: 0.032908\n",
      "Train Epoch: Model Number: 1 0 [10240/60000 (17%)]\tLoss: 0.035336\n",
      "Train Epoch: Model Number: 1 0 [10304/60000 (17%)]\tLoss: 0.025428\n",
      "Train Epoch: Model Number: 1 0 [10368/60000 (17%)]\tLoss: 0.029170\n",
      "Train Epoch: Model Number: 1 0 [10432/60000 (17%)]\tLoss: 0.035446\n",
      "Train Epoch: Model Number: 1 0 [10496/60000 (17%)]\tLoss: 0.018303\n",
      "Train Epoch: Model Number: 1 0 [10560/60000 (18%)]\tLoss: 0.013594\n",
      "Train Epoch: Model Number: 1 0 [10624/60000 (18%)]\tLoss: 0.027365\n",
      "Train Epoch: Model Number: 1 0 [10688/60000 (18%)]\tLoss: 0.022575\n",
      "Train Epoch: Model Number: 1 0 [10752/60000 (18%)]\tLoss: 0.030966\n",
      "Train Epoch: Model Number: 1 0 [10816/60000 (18%)]\tLoss: 0.035561\n",
      "Train Epoch: Model Number: 1 0 [10880/60000 (18%)]\tLoss: 0.034044\n",
      "Train Epoch: Model Number: 1 0 [10944/60000 (18%)]\tLoss: 0.025980\n",
      "Train Epoch: Model Number: 1 0 [11008/60000 (18%)]\tLoss: 0.021783\n",
      "Train Epoch: Model Number: 1 0 [11072/60000 (18%)]\tLoss: 0.025273\n",
      "Train Epoch: Model Number: 1 0 [11136/60000 (19%)]\tLoss: 0.031318\n",
      "Train Epoch: Model Number: 1 0 [11200/60000 (19%)]\tLoss: 0.015026\n",
      "Train Epoch: Model Number: 1 0 [11264/60000 (19%)]\tLoss: 0.016035\n",
      "Train Epoch: Model Number: 1 0 [11328/60000 (19%)]\tLoss: 0.021281\n",
      "Train Epoch: Model Number: 1 0 [11392/60000 (19%)]\tLoss: 0.025858\n",
      "Train Epoch: Model Number: 1 0 [11456/60000 (19%)]\tLoss: 0.033730\n",
      "Train Epoch: Model Number: 1 0 [11520/60000 (19%)]\tLoss: 0.032056\n",
      "Train Epoch: Model Number: 1 0 [11584/60000 (19%)]\tLoss: 0.027656\n",
      "Train Epoch: Model Number: 1 0 [11648/60000 (19%)]\tLoss: 0.030120\n",
      "Train Epoch: Model Number: 1 0 [11712/60000 (20%)]\tLoss: 0.021679\n",
      "Train Epoch: Model Number: 1 0 [11776/60000 (20%)]\tLoss: 0.026482\n",
      "Train Epoch: Model Number: 1 0 [11840/60000 (20%)]\tLoss: 0.024178\n",
      "Train Epoch: Model Number: 1 0 [11904/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [11968/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12032/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12096/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12160/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12224/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12288/60000 (20%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12352/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12416/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12480/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12544/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12608/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12672/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12736/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12800/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12864/60000 (21%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12928/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [12992/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13056/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13120/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13184/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13248/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13312/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13376/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13440/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13504/60000 (22%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13568/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13632/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13696/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13760/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13824/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13888/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [13952/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14016/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14080/60000 (23%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14144/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14208/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14272/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14336/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14400/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14464/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14528/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14592/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14656/60000 (24%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14720/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14784/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14848/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14912/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [14976/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15040/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15104/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15168/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15232/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15296/60000 (25%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15360/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15424/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15488/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15552/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15616/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15680/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15744/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15808/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15872/60000 (26%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [15936/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16000/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16064/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16128/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16192/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16256/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16320/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16384/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16448/60000 (27%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16512/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16576/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16640/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16704/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16768/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16832/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16896/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [16960/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17024/60000 (28%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17088/60000 (28%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [17152/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17216/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17280/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17344/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17408/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17472/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17536/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17600/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17664/60000 (29%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17728/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17792/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17856/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17920/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [17984/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18048/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18112/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18176/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18240/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18304/60000 (30%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18368/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18432/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18496/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18560/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18624/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18688/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18752/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18816/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18880/60000 (31%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [18944/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19008/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19072/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19136/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19200/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19264/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19328/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19392/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19456/60000 (32%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19520/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19584/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19648/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19712/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19776/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19840/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19904/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [19968/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20032/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20096/60000 (33%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20160/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20224/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20288/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20352/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20416/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20480/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20544/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20608/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20672/60000 (34%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20736/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20800/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20864/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20928/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [20992/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21056/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21120/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21184/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21248/60000 (35%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21312/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21376/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21440/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21504/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21568/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21632/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21696/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21760/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21824/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21888/60000 (36%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [21952/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22016/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22080/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22144/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22208/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22272/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22336/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22400/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22464/60000 (37%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22528/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22592/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22656/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22720/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22784/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22848/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22912/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [22976/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23040/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23104/60000 (38%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23168/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23232/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23296/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23360/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23424/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23488/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23552/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23616/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23680/60000 (39%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23744/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23808/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23872/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [23936/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24000/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24064/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24128/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24192/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24256/60000 (40%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24320/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24384/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24448/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24512/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24576/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24640/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24704/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24768/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24832/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24896/60000 (41%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [24960/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25024/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25088/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25152/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25216/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25280/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25344/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25408/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25472/60000 (42%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25536/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25600/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25664/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25728/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25792/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25856/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25920/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [25984/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26048/60000 (43%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [26112/60000 (43%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26176/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26240/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26304/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26368/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26432/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26496/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26560/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26624/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26688/60000 (44%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26752/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26816/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26880/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [26944/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27008/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27072/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27136/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27200/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27264/60000 (45%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27328/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27392/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27456/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27520/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27584/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27648/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27712/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27776/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27840/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27904/60000 (46%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [27968/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28032/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28096/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28160/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28224/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28288/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28352/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28416/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28480/60000 (47%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28544/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28608/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28672/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28736/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28800/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28864/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28928/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [28992/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29056/60000 (48%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29120/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29184/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29248/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29312/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29376/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29440/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29504/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29568/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29632/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29696/60000 (49%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29760/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29824/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29888/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [29952/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30016/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30080/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30144/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30208/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30272/60000 (50%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30336/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30400/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30464/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30528/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30592/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30656/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30720/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30784/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30848/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30912/60000 (51%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [30976/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31040/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31104/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31168/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31232/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31296/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31360/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31424/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31488/60000 (52%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31552/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31616/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31680/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31744/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31808/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31872/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [31936/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32000/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32064/60000 (53%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32128/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32192/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32256/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32320/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32384/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32448/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32512/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32576/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32640/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32704/60000 (54%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32768/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32832/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32896/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [32960/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33024/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33088/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33152/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33216/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33280/60000 (55%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33344/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33408/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33472/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33536/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33600/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33664/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33728/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33792/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33856/60000 (56%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33920/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [33984/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34048/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34112/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34176/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34240/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34304/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34368/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34432/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34496/60000 (57%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34560/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34624/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34688/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34752/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34816/60000 (58%)]\tLoss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [34880/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [34944/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [35008/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [35072/60000 (58%)]\tLoss: nan\n",
      "Train Epoch: Model Number: 1 0 [35136/60000 (59%)]\tLoss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6caa99b6db02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_of_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoosting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-6b8afcf9987b>\u001b[0m in \u001b[0;36mGradientBoosting\u001b[1;34m(initial_model, M)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mtarget_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_digits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mtarget_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_onehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models, gamma_exp = GradientBoosting(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Load submodels and optimized gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the models trained\n",
    "for i in range(num_of_models):\n",
    "    model = models[i]\n",
    "    torch.save(model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized gamma\n",
    "torch.save(gamma_exp, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from the local files\n",
    "num_of_models = 3\n",
    "models = []\n",
    "for x in range(num_of_models):\n",
    "    globals()['model%s' % x] = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(1) + '.pth')\n",
    "    models.append(globals()['model%s' % x])\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized gamma from the local files\n",
    "gamma_exp = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')\n",
    "print(gamma_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cozyn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 1.5499, Accuracy: 54379/60000 (90.632%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the ensemble model\n",
    "initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()\n",
    "\n",
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)  \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW Attack on Intial Model, Testing Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 69.22 %\n"
     ]
    }
   ],
   "source": [
    "# Attack the initial model using CW attack\n",
    "initial_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW(initial_model, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ensemble CW Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble CW attack\n",
    "class Attack_ensemble(object):\n",
    "    r\"\"\"\n",
    "    Base class for all attacks.\n",
    "    .. note::\n",
    "        It automatically set device to the device where given model is.\n",
    "        It temporarily changes the original model's training mode to `test`\n",
    "        by `.eval()` only during an attack process.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, model, models, gamma):\n",
    "        r\"\"\"\n",
    "        Initializes internal attack state.\n",
    "        Arguments:\n",
    "            name (str) : name of an attack.\n",
    "            model (torch.nn.Module): model to attack.\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack = name\n",
    "        self.model = model\n",
    "        self.models = models\n",
    "        self.gamma = gamma\n",
    "        self.model_name = str(model).split(\"(\")[0]\n",
    "\n",
    "        self.training = model.training\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        self._targeted = 1\n",
    "        self._attack_mode = 'original'\n",
    "        self._return_type = 'float'\n",
    "\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"\n",
    "        It defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_attack_mode(self, mode):\n",
    "        r\"\"\"\n",
    "        Set the attack mode.\n",
    "  \n",
    "        Arguments:\n",
    "            mode (str) : 'original' (DEFAULT)\n",
    "                         'targeted' - Use input labels as targeted labels.\n",
    "                         'least_likely' - Use least likely labels as targeted labels.\n",
    "        \"\"\"\n",
    "        if self._attack_mode is 'only_original':\n",
    "            raise ValueError(\"Changing attack mode is not supported in this attack method.\")\n",
    "            \n",
    "        if mode==\"original\":\n",
    "            self._attack_mode = \"original\"\n",
    "            self._targeted = 1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"targeted\":\n",
    "            self._attack_mode = \"targeted\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"least_likely\":\n",
    "            self._attack_mode = \"least_likely\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_least_likely_label\n",
    "        else:\n",
    "            raise ValueError(mode + \" is not a valid mode. [Options : original, targeted, least_likely]\")\n",
    "            \n",
    "    def set_return_type(self, type):\n",
    "        r\"\"\"\n",
    "        Set the return type of adversarial images: `int` or `float`.\n",
    "        Arguments:\n",
    "            type (str) : 'float' or 'int'. (DEFAULT : 'float')\n",
    "        \"\"\"\n",
    "        if type == 'float':\n",
    "            self._return_type = 'float'\n",
    "        elif type == 'int':\n",
    "            self._return_type = 'int'\n",
    "        else:\n",
    "            raise ValueError(type + \" is not a valid type. [Options : float, int]\")\n",
    "\n",
    "    def save(self, save_path, data_loader, verbose=True):\n",
    "        r\"\"\"\n",
    "        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.\n",
    "        Arguments:\n",
    "            save_path (str) : save_path.\n",
    "            data_loader (torch.utils.data.DataLoader) : data loader.\n",
    "            verbose (bool) : True for displaying detailed information. (DEFAULT : True)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        total_batch = len(data_loader)\n",
    "\n",
    "        for step, (images, labels) in enumerate(data_loader):\n",
    "            adv_images = self.__call__(images, labels)\n",
    "\n",
    "            image_list.append(adv_images.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "            if self._return_type == 'int':\n",
    "                adv_images = adv_images.float()/255\n",
    "\n",
    "            if verbose:\n",
    "                outputs = self.model(adv_images)\n",
    "                for i in range(len(self.models)):\n",
    "                    sub_model = self.models[i]\n",
    "                    outputs = outputs + self.gamma[i] * sub_model(adv_images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(self.device)).sum()\n",
    "\n",
    "                acc = 100 * float(correct) / total\n",
    "                print('- Save Progress : %2.2f %% / Accuracy : %2.2f %%' % ((step+1)/total_batch*100, acc), end='\\r')\n",
    "\n",
    "        x = torch.cat(image_list, 0)\n",
    "        y = torch.cat(label_list, 0)\n",
    "        torch.save((x, y), save_path)\n",
    "        print('\\n- Save Complete!')\n",
    "\n",
    "        self._switch_model()\n",
    "        \n",
    "    def _transform_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "        \n",
    "    def _get_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return input labels.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "    \n",
    "    def _get_least_likely_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return least likely labels.\n",
    "        \"\"\"\n",
    "        outputs = self.model(images)\n",
    "        for i in range(len(self.models)):\n",
    "            sub_model = self.models[i]\n",
    "            outputs = outputs + self.gamma[i] * sub_model(images)\n",
    "        _, labels = torch.min(outputs.data, 1)\n",
    "        labels = labels.detach_()\n",
    "        return labels\n",
    "    \n",
    "    def _to_uint(self, images):\n",
    "        r\"\"\"\n",
    "        Function for changing the return type.\n",
    "        Return images as int.\n",
    "        \"\"\"\n",
    "        return (images*255).type(torch.uint8)\n",
    "\n",
    "    def _switch_model(self):\n",
    "        r\"\"\"\n",
    "        Function for changing the training mode of the model.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.model.train()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].eval()\n",
    "\n",
    "    def __str__(self):\n",
    "        info = self.__dict__.copy()\n",
    "        \n",
    "        del_keys = ['model', 'attack']\n",
    "        \n",
    "        for key in info.keys():\n",
    "            if key[0] == \"_\" :\n",
    "                del_keys.append(key)\n",
    "                \n",
    "        for key in del_keys:\n",
    "            del info[key]\n",
    "        \n",
    "        info['attack_mode'] = self._attack_mode\n",
    "        if info['attack_mode'] == 'only_original' :\n",
    "            info['attack_mode'] = 'original'\n",
    "            \n",
    "        info['return_type'] = self._return_type\n",
    "        \n",
    "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
    "\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        self.model.eval()\n",
    "        for i in range(len(self.models)):\n",
    "            self.models[i].eval()\n",
    "        images = self.forward(*input, **kwargs)\n",
    "        self._switch_model()\n",
    "\n",
    "        if self._return_type == 'int':\n",
    "            images = self._to_uint(images)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ensemble CW Attack\n",
    "import warnings\n",
    "\n",
    "class CW_Ensemble(Attack_ensemble):\n",
    "    r\"\"\"\n",
    "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
    "    [https://arxiv.org/abs/1608.04644]\n",
    "    Distance Measure : L2\n",
    "        \n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        c (float): c in the paper. parameter for box-constraint. (DEFALUT : 1e-4)    \n",
    "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`    \n",
    "        kappa (float): kappa (also written as 'confidence') in the paper. (DEFALUT : 0)\n",
    "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
    "        steps (int): number of steps. (DEFALUT : 1000)\n",
    "        lr (float): learning rate of the Adam optimizer. (DEFALUT : 0.01)\n",
    "        \n",
    "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
    "    \n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "          \n",
    "    Examples::\n",
    "        >>> attack = torchattacks.CW(model, targeted=False, c=1e-4, kappa=0, steps=1000, lr=0.01)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "        \n",
    "    .. note:: NOT IMPLEMENTED methods in the paper due to time consuming.\n",
    "    \n",
    "        (1) Binary search for c.\n",
    "        \n",
    "        (2) Choosing best L2 adversaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, models, gamma, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super(CW_Ensemble, self).__init__(\"CW\", model, models, gamma)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        labels = self._transform_label(images, labels)\n",
    "\n",
    "        # f-function in the paper\n",
    "        def f(x):\n",
    "            outputs = self.model(x)\n",
    "            for i in range(len(self.models)):\n",
    "                sub_model = self.models[i]\n",
    "                outputs = outputs + self.gamma[i] * sub_model(x)\n",
    "            one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "            i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "            j = torch.masked_select(outputs, one_hot_labels.bool())\n",
    "\n",
    "            return torch.clamp(self._targeted*(j-i), min=-self.kappa)\n",
    "\n",
    "        w = torch.zeros_like(images).to(self.device)\n",
    "        w.detach_()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "        prev = 1e10\n",
    "\n",
    "        for step in range(self.steps):\n",
    "\n",
    "            a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "            loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "            loss2 = torch.sum(self.c*f(a))\n",
    "\n",
    "            cost = loss1 + loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stop when loss does not converge.\n",
    "            if step % (self.steps//10) == 0:\n",
    "                if cost > prev:\n",
    "                    warnings.warn(\"Early stopped because the loss is not converged.\")\n",
    "                    return (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "                prev = cost\n",
    "\n",
    "            # print('- CW Attack Progress : %2.2f %%        ' %((step+1)/self.steps*100), end='\\r')\n",
    "\n",
    "        adv_images = (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "\n",
    "        return adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW Attack Ensemble Model, Testing Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 70.03 %\n"
     ]
    }
   ],
   "source": [
    "#Attack the ensemble model\n",
    "initial_model.eval()\n",
    "for i in range(num_of_models):\n",
    "    models[i].eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW_Ensemble(model = initial_model,models = models,gamma = gamma_exp, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        for i in range(num_of_models):\n",
    "            sub_model = models[i]\n",
    "            outputs = outputs + gamma_exp[i] * sub_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
