{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import CW\n",
    "# from CW_Emsemble_Attack import CW as CW_ensemble\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(HNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "#         x = F.softmax(x, dim = 1)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NHNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_exp = []\n",
    "train_output = []\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "# test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = HNet()\n",
    "\n",
    "optimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_model = initial_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    initial_model.train()\n",
    "#     exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = initial_model(data)\n",
    "#         print(output.shape)\n",
    "#         train_output.append(output)\n",
    "#         if batch_idx == 937:      \n",
    "#             train_output.append(output)\n",
    "#         print(\"before:\",batch_idx,output[0])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(optimizer.state_dict())\n",
    "#         gamma_exp.append(optimizer.state_dict()['exp_avg'])\n",
    "#         gamma_exp_sq.append(optimizer.state_dict()['exp_avg_sq'])\n",
    "#         print(gamma[''])\n",
    "#         print(\"after:\",output[0])\n",
    "#         train_output.append(output.data.max(1, keepdim=True))\n",
    "#         if batch_idx == 0:\n",
    "#             print(output.data.max(1, keepdim=True)[1].shape)\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        torch.save(initial_model.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model.pth')\n",
    "        torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/optimizer.pth')\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "#             train_losses.append(loss.item())\n",
    "#             train_counter.append(\n",
    "#                 (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    initial_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "#             data, target = Variable(data, volatile=True), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = initial_model(data)\n",
    "        \n",
    "            loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    test_losses.append(loss)    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.392823\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.570041\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.565410\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.234962\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.348309\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.191354\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.249863\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.202573\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.170296\n",
      "\n",
      "Average loss: 0.1745, Accuracy: 56759/60000 (94.598%)\n",
      "\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.088725\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.254969\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.149976\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.128624\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.285364\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.102782\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.336375\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.147386\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.167869\n",
      "\n",
      "Average loss: 0.1335, Accuracy: 57509/60000 (95.848%)\n",
      "\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.121809\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093731\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.070540\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.315899\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.159642\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.074147\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.081529\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.203864\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.041307\n",
      "\n",
      "Average loss: 0.1204, Accuracy: 57699/60000 (96.165%)\n",
      "\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.081878\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.142022\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.222509\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.094306\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.109478\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.311767\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.128522\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.106066\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.240527\n",
      "\n",
      "Average loss: 0.1375, Accuracy: 57383/60000 (95.638%)\n",
      "\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.233833\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.023407\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.083684\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.157274\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.085340\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.133199\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.025699\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.054158\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.203519\n",
      "\n",
      "Average loss: 0.1592, Accuracy: 56995/60000 (94.992%)\n",
      "\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.010809\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.064718\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.047537\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.076468\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.024767\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.135958\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.074194\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.273586\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.104580\n",
      "\n",
      "Average loss: 0.0872, Accuracy: 58280/60000 (97.133%)\n",
      "\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.130979\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.109843\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.045167\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.062689\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.056971\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.029280\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.120412\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.057093\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.189639\n",
      "\n",
      "Average loss: 0.0827, Accuracy: 58388/60000 (97.313%)\n",
      "\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.080316\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.079429\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.025858\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.060107\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.127678\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.068307\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.079466\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.209026\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.029647\n",
      "\n",
      "Average loss: 0.0836, Accuracy: 58374/60000 (97.290%)\n",
      "\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.014948\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.052629\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.042829\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005297\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.012879\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.124770\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.045717\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.170987\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.061755\n",
      "\n",
      "Average loss: 0.0965, Accuracy: 58152/60000 (96.920%)\n",
      "\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.164333\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.045165\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.171126\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.033798\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.008572\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.090159\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.030200\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.204235\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.052840\n",
      "\n",
      "Average loss: 0.0675, Accuracy: 58694/60000 (97.823%)\n",
      "\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.018823\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002986\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.072105\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.066589\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.023393\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.062920\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.065253\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.134163\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.107628\n",
      "\n",
      "Average loss: 0.0713, Accuracy: 58673/60000 (97.788%)\n",
      "\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.049973\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.073572\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.107050\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.102327\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.029012\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.126265\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.006376\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.059618\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.090159\n",
      "\n",
      "Average loss: 0.0565, Accuracy: 58925/60000 (98.208%)\n",
      "\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.054929\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.223312\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.020028\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.018795\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.007189\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.056808\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.131099\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.116880\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.016847\n",
      "\n",
      "Average loss: 0.0702, Accuracy: 58708/60000 (97.847%)\n",
      "\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.058700\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.025208\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.046189\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.107915\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.079311\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.052579\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.064906\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.140618\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.118383\n",
      "\n",
      "Average loss: 0.0709, Accuracy: 58755/60000 (97.925%)\n",
      "\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.000525\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.014095\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.024814\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.045176\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.006274\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.069658\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.083997\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.008163\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.012550\n",
      "\n",
      "Average loss: 0.0569, Accuracy: 58887/60000 (98.145%)\n",
      "\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.084889\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.060696\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.037582\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.024352\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.011525\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.058981\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.017906\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.104572\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.046055\n",
      "\n",
      "Average loss: 0.0576, Accuracy: 58858/60000 (98.097%)\n",
      "\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.020279\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.005344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.030013\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.053026\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.023321\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.091302\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.028209\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.047958\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.079661\n",
      "\n",
      "Average loss: 0.0608, Accuracy: 58890/60000 (98.150%)\n",
      "\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.037211\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.018008\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.027271\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.028238\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.214740\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.025784\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.049863\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.001453\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.031715\n",
      "\n",
      "Average loss: 0.0548, Accuracy: 59007/60000 (98.345%)\n",
      "\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.000828\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.048641\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.082269\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.006944\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.037860\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.041802\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.078353\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.165829\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.074182\n",
      "\n",
      "Average loss: 0.0842, Accuracy: 58435/60000 (97.392%)\n",
      "\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.036712\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.033679\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.057958\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.011560\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.059208\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.020913\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.010497\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.075814\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.091499\n",
      "\n",
      "Average loss: 0.0564, Accuracy: 58994/60000 (98.323%)\n",
      "\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.236793\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.100458\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.076195\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.070209\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.045740\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.008874\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.033982\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.005932\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.053872\n",
      "\n",
      "Average loss: 0.0498, Accuracy: 59076/60000 (98.460%)\n",
      "\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.040289\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.001025\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.050512\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.009719\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.021273\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.013926\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.023315\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.018460\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.148021\n",
      "\n",
      "Average loss: 0.0377, Accuracy: 59266/60000 (98.777%)\n",
      "\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.006842\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.065381\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.018066\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.012260\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.048173\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.080178\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.017376\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.063433\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.052846\n",
      "\n",
      "Average loss: 0.0293, Accuracy: 59429/60000 (99.048%)\n",
      "\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.072236\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.022748\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.051427\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.015577\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.003249\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.021872\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.001385\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.086292\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.088569\n",
      "\n",
      "Average loss: 0.0690, Accuracy: 58753/60000 (97.922%)\n",
      "\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.048776\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.029522\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.079362\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.011137\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.062232\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.089949\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.114501\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.002532\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.053434\n",
      "\n",
      "Average loss: 0.0521, Accuracy: 58987/60000 (98.312%)\n",
      "\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.019843\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.002555\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.024058\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.014101\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.034973\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.004605\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.010639\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.007868\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.000229\n",
      "\n",
      "Average loss: 0.0443, Accuracy: 59172/60000 (98.620%)\n",
      "\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 0.013559\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.006837\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.075782\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.029045\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 0.004700\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.060640\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 0.090294\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.276911\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.054412\n",
      "\n",
      "Average loss: 0.0492, Accuracy: 59134/60000 (98.557%)\n",
      "\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 0.037246\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.101398\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.060363\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.172592\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 0.085347\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.049465\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 0.003611\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.000271\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.003648\n",
      "\n",
      "Average loss: 0.0456, Accuracy: 59176/60000 (98.627%)\n",
      "\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 0.007998\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.020217\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.050204\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.005288\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 0.059890\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.081226\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 0.127697\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.008109\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.002526\n",
      "\n",
      "Average loss: 0.0587, Accuracy: 59115/60000 (98.525%)\n",
      "\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 0.001176\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.008061\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.010119\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.093415\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 0.000581\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.003393\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 0.044773\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.021973\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.159789\n",
      "\n",
      "Average loss: 0.0403, Accuracy: 59240/60000 (98.733%)\n",
      "\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 0.025291\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.086860\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.079283\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.084519\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 0.068398\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.012549\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 0.130047\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.006866\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.048476\n",
      "\n",
      "Average loss: 0.0821, Accuracy: 58621/60000 (97.702%)\n",
      "\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 0.057774\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.029968\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.086338\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.213542\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 0.001097\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.040042\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 0.004503\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.068187\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.051086\n",
      "\n",
      "Average loss: 0.0271, Accuracy: 59492/60000 (99.153%)\n",
      "\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 0.072546\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.032360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.045142\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.110974\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 0.016292\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.028717\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 0.033552\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.000558\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.156964\n",
      "\n",
      "Average loss: 0.0498, Accuracy: 59062/60000 (98.437%)\n",
      "\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 0.019424\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.200248\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.081500\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.008602\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 0.064084\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.129034\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 0.001996\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.021996\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.160991\n",
      "\n",
      "Average loss: 0.0475, Accuracy: 59150/60000 (98.583%)\n",
      "\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 0.013164\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.001963\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.085543\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.001635\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 0.001193\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.009332\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 0.022307\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.003856\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.007593\n",
      "\n",
      "Average loss: 0.0452, Accuracy: 59184/60000 (98.640%)\n",
      "\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 0.100961\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.011985\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.254690\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.012339\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 0.016722\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.045738\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 0.056244\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.008441\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.008097\n",
      "\n",
      "Average loss: 0.0484, Accuracy: 59192/60000 (98.653%)\n",
      "\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 0.000391\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.005275\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 0.096876\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.000321\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 0.033076\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.128317\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 0.014829\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.019290\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 0.036156\n",
      "\n",
      "Average loss: 0.0387, Accuracy: 59330/60000 (98.883%)\n",
      "\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 0.137151\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.011948\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 0.025690\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.032197\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 0.003075\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.000597\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 0.081202\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.027808\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 0.003824\n",
      "\n",
      "Average loss: 0.0562, Accuracy: 59078/60000 (98.463%)\n",
      "\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 0.031501\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.067630\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 0.003067\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.013334\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 0.001519\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.019576\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 0.034299\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.068126\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 0.043282\n",
      "\n",
      "Average loss: 0.0335, Accuracy: 59377/60000 (98.962%)\n",
      "\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 0.003248\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.001227\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 0.001133\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.000363\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 0.109867\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.000366\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 0.005466\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.072002\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 0.012741\n",
      "\n",
      "Average loss: 0.0320, Accuracy: 59428/60000 (99.047%)\n",
      "\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 0.002598\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.086644\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 0.012635\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.012384\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 0.151500\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.001532\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 0.046686\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.021142\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 0.000797\n",
      "\n",
      "Average loss: 0.0291, Accuracy: 59519/60000 (99.198%)\n",
      "\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 0.004832\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 0.001524\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 0.017340\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.021266\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 0.083984\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 0.004539\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 0.049352\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.190026\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 0.103663\n",
      "\n",
      "Average loss: 0.0553, Accuracy: 59134/60000 (98.557%)\n",
      "\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 0.003488\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 0.059875\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 0.006892\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.087491\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 0.010477\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 0.020773\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 0.021835\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.001759\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 0.065224\n",
      "\n",
      "Average loss: 0.0393, Accuracy: 59302/60000 (98.837%)\n",
      "\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 0.139815\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 0.066464\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 0.020373\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.049939\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 0.003668\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 0.136342\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 0.023418\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.019190\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 0.054927\n",
      "\n",
      "Average loss: 0.0211, Accuracy: 59612/60000 (99.353%)\n",
      "\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 0.002274\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 0.013924\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 0.010693\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.021082\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 0.010725\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 0.002594\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 0.000140\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.084633\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 0.053858\n",
      "\n",
      "Average loss: 0.0360, Accuracy: 59372/60000 (98.953%)\n",
      "\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 0.251273\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 0.059048\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 0.000506\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.008100\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 0.027776\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.018246\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 0.070743\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.098053\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 0.000734\n",
      "\n",
      "Average loss: 0.0244, Accuracy: 59584/60000 (99.307%)\n",
      "\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 0.000124\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 0.073766\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 0.006032\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.092754\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 0.056038\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 0.041656\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 0.051982\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.002139\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 0.141924\n",
      "\n",
      "Average loss: 0.0247, Accuracy: 59546/60000 (99.243%)\n",
      "\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 0.064310\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 0.005904\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 0.001410\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.000615\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 0.187205\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 0.010616\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 0.097118\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.038126\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 0.003249\n",
      "\n",
      "Average loss: 0.0154, Accuracy: 59672/60000 (99.453%)\n",
      "\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 0.072766\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 0.008308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 0.002668\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.056229\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 0.144359\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 0.000561\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 0.022782\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.017923\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 0.018786\n",
      "\n",
      "Average loss: 0.0292, Accuracy: 59490/60000 (99.150%)\n",
      "\n",
      "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 0.014171\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 0.002452\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 0.000408\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.060767\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 0.221991\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 0.195491\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 0.090293\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.132819\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 0.046763\n",
      "\n",
      "Average loss: 0.0142, Accuracy: 59724/60000 (99.540%)\n",
      "\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 0.087692\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 0.000120\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 0.009141\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.120217\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 0.060151\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.014808\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 0.000065\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.001352\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 0.172485\n",
      "\n",
      "Average loss: 0.0332, Accuracy: 59449/60000 (99.082%)\n",
      "\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 0.047347\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 0.044765\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 0.295513\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 0.018815\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 0.053657\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 0.001125\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 0.000767\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 0.000065\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 0.071887\n",
      "\n",
      "Average loss: 0.0204, Accuracy: 59640/60000 (99.400%)\n",
      "\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 0.003047\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 0.001454\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 0.003262\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 0.000041\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 0.055783\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 0.195268\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 0.002071\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 0.180402\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 0.000336\n",
      "\n",
      "Average loss: 0.0344, Accuracy: 59447/60000 (99.078%)\n",
      "\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 0.012445\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 0.000730\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 0.000155\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 0.005032\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 0.003224\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 0.001194\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 0.003904\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 0.072269\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 0.008992\n",
      "\n",
      "Average loss: 0.0205, Accuracy: 59626/60000 (99.377%)\n",
      "\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 0.006784\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 0.000431\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 0.006893\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 0.039093\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 0.000629\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 0.001280\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 0.002849\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 0.016921\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 0.102556\n",
      "\n",
      "Average loss: 0.0783, Accuracy: 58931/60000 (98.218%)\n",
      "\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 0.021690\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 0.234258\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 0.000076\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 0.001839\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 0.035853\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 0.112219\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 0.039110\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 0.016249\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 0.003562\n",
      "\n",
      "Average loss: 0.0624, Accuracy: 59109/60000 (98.515%)\n",
      "\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 0.011481\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 0.002633\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 0.000054\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 0.050536\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 0.000690\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 0.004705\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 0.017447\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 0.023687\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 0.081115\n",
      "\n",
      "Average loss: 0.0347, Accuracy: 59447/60000 (99.078%)\n",
      "\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 0.001536\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 0.001559\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 0.000814\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 0.005370\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 0.111292\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 0.089593\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 0.053541\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 0.095456\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 0.215168\n",
      "\n",
      "Average loss: 0.0424, Accuracy: 59332/60000 (98.887%)\n",
      "\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 0.023633\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 0.003359\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 0.003550\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 0.001169\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 0.004365\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 0.189370\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 0.080688\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 0.000769\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 0.018361\n",
      "\n",
      "Average loss: 0.0346, Accuracy: 59493/60000 (99.155%)\n",
      "\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 0.013314\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 0.100584\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 0.006857\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 0.072065\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 0.026073\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 0.003132\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 0.018275\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 0.039612\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 0.000978\n",
      "\n",
      "Average loss: 0.0471, Accuracy: 59335/60000 (98.892%)\n",
      "\n",
      "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 0.000369\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 0.001698\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 0.120844\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 0.004845\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 0.004350\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 0.009521\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 0.174700\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 0.000049\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 0.081102\n",
      "\n",
      "Average loss: 0.0293, Accuracy: 59507/60000 (99.178%)\n",
      "\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 0.027413\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 0.012351\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 0.000216\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 0.003756\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 0.105770\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 0.001169\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 0.088549\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 0.018402\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 0.018352\n",
      "\n",
      "Average loss: 0.0245, Accuracy: 59563/60000 (99.272%)\n",
      "\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 0.074725\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 0.000255\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 0.076320\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 0.008366\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 0.230106\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 0.129901\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 0.008045\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 0.036708\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 0.033039\n",
      "\n",
      "Average loss: 0.0377, Accuracy: 59425/60000 (99.042%)\n",
      "\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 0.038527\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 0.002410\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 0.004961\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 0.001086\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 0.020012\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 0.025919\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 0.208196\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 0.002519\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 0.102535\n",
      "\n",
      "Average loss: 0.0168, Accuracy: 59703/60000 (99.505%)\n",
      "\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 0.006134\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 0.048928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 0.073163\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 0.014512\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 0.009101\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 0.020023\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 0.000291\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 0.009595\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 0.000244\n",
      "\n",
      "Average loss: 0.0217, Accuracy: 59609/60000 (99.348%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 65\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HNet(\n",
       "  (flatten): Flatten()\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model = torch.load('C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseresidual(y, F):\n",
    "    return y - F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hoptimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "Hcriterion = nn.MSELoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Hcriterion = Hcriterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0217, Accuracy: 59609/60000 (99.348%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Htrain(Hmodel, epoch):\n",
    "    Hmodel.train()\n",
    "    for m in range(num_of_models):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            nb_digits = 10\n",
    "            target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "                target_onehot = target_onehot.cuda()\n",
    "            Hoptimizer.zero_grad()\n",
    "            output = initial_model(data)\n",
    "            for i in range(m):\n",
    "                model = models[i]\n",
    "                if torch.cuda.is_available():\n",
    "                    output = output.cuda()\n",
    "                    model = model.cuda()\n",
    "                output = output + gamma_exp[i] * model(data)\n",
    "            target = target.view(-1,1)\n",
    "            target_onehot.zero_()\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            residual = mseresidual(target_onehot, output)\n",
    "            houtput = Hmodel(data)\n",
    "            houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "            residual = residual.type(torch.cuda.FloatTensor)\n",
    "#             residual_list.append(residual)\n",
    "    #             print(\"residual is:\", residual)\n",
    "    #             print(\"predicted is:\", houtput)\n",
    "            loss = Hcriterion(houtput, residual)\n",
    "            loss.backward(retain_graph=True)\n",
    "            Hoptimizer.step()\n",
    "            if (batch_idx + 1)% 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoosting(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64)\n",
    "    models = []\n",
    "    residual_list = []\n",
    "    for m in range(M):\n",
    "        # Create new model for training residuals\n",
    "        Hmodel = NHNet()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "#         Htrain(Hmodel, 100)\n",
    "#         for i in range(5):\n",
    "#             print(i)\n",
    "        Hmodel.train()\n",
    "        epoch = 3\n",
    "        for i in range(epoch):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                Hoptimizer.zero_grad()\n",
    "                output = initial_model(data)\n",
    "                for i in range(m):\n",
    "                    model = models[i]\n",
    "                    if torch.cuda.is_available():\n",
    "                        output = output.cuda()\n",
    "                        model = model.cuda()\n",
    "                    output = output + gamma_exp[i] * model(data)\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                residual = mseresidual(target_onehot, output)\n",
    "                houtput = Hmodel(data)\n",
    "                houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                residual = residual.type(torch.cuda.FloatTensor)\n",
    "                residual_list.append(residual)\n",
    "    #             print(\"residual is:\", residual)\n",
    "    #             print(\"predicted is:\", houtput)\n",
    "                loss = Hcriterion(houtput, residual)\n",
    "                loss.backward(retain_graph=True)\n",
    "                Hoptimizer.step()\n",
    "                if (batch_idx + 1)% 100 == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                        100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "#             print('batch_idx', batch_idx)\n",
    "#             train_losses.append(loss.item())\n",
    "#             torch.save(model.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/model.pth')\n",
    "#             torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/optimizer.pth')\n",
    "        models.append(Hmodel)\n",
    "#         print(\"Hmodel is:\", Hmodel)\n",
    "#         print(\"Appended model is:\", models[1])\n",
    "#         print(\"Length of models:\", len(models))\n",
    "        \n",
    "        \n",
    "#         loss = 0\n",
    "#         correct = 0\n",
    "    \n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, (data, target) in enumerate(train_loader):\n",
    "# #             data, target = Variable(data, volatile=True), Variable(target)\n",
    "#                 residual = residual_list[batch_idx]\n",
    "#                 if torch.cuda.is_available():\n",
    "#                     data = data.cuda()\n",
    "#                     residual = residual.cuda()\n",
    "#                 residual = residual.type(torch.cuda.LongTensor)\n",
    "#                 output = Hmodel(data)\n",
    "#                 loss += F.cross_entropy(output, residual, reduction='sum').item()\n",
    "#                 pred = output.data.max(1, keepdim=True)[1]\n",
    "#                 correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "#         loss /= len(residual_list)\n",
    "#     # test_losses.append(loss)    \n",
    "#         print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "#             loss, correct, len(residual_list),\n",
    "#             100. * correct / len(residual_list)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "#         gamma[0] = 0.1\n",
    "        print(\"Initialized gamma:\", gamma)\n",
    "#         Variable(gamma)\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.003)\n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                for i in range(m):\n",
    "                    model = models[i]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[i]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                temp = Hmodel(data)\n",
    "    #             print('output is:', output)\n",
    "    #             print('gamma is:', gamma.shape)\n",
    "    #             print('Hmodel(data) is:', temp)\n",
    "                predicted = output + gamma * temp\n",
    "    #             print(\"predicted is:\", predicted)\n",
    "    #             predicted.double()\n",
    "    #             target_onehot.double()\n",
    "                loss = Hcriterion(predicted, target_onehot)\n",
    "    #             print(\"target_onehot is:\", target_onehot)\n",
    "    #             train_losses.append(loss.item())\n",
    "    #             torch.save(model.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/model.pth')\n",
    "    #             torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/optimizer.pth')\n",
    "    #             print(\"loss is:\", loss)\n",
    "                loss.backward(retain_graph=True)\n",
    "#                 print(\"gamma is before:\", gamma)\n",
    "    #             print(\"gamma's gradient is:\", gamma.retain_grad())\n",
    "                Goptimizer.step()\n",
    "#                 print(\"gamma is after:\", gamma)\n",
    "        gamma_exp[m] = gamma\n",
    "        print(gamma_exp)\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.443060\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.650797\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.920788\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.877415\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.571002\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.437952\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.350985\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.311539\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.232324\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.198637\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.152882\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.137214\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.117305\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.135385\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.109400\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.100612\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.094715\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.093543\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.090856\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.089436\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090086\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.090697\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.090233\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.090284\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.090232\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.145677\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.089946\n",
      "Initialized gamma: tensor([0.2921], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0040, 1.0000, 1.0000], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.093255\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.091260\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090005\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089714\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.090281\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.089710\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.090271\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.090134\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.090285\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.089700\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.090040\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090135\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089480\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.089731\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.089828\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089851\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.090079\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.090043\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.089743\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.089931\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.089887\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089618\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.089943\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.090069\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089997\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.089727\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.089991\n",
      "Initialized gamma: tensor([0.3568], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0040, 0.9936, 1.0000], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.090099\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.089994\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090070\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.090096\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.089924\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.090066\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089969\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.089946\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.090171\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.090227\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.089510\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.089867\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.090503\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.090159\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.089741\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089955\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.090176\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.090227\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.090088\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.090348\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090346\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089947\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.090031\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.089971\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.090226\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.089880\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.090312\n",
      "Initialized gamma: tensor([0.3610], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0040, 0.9936, 1.0037], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models, gamma_exp = GradientBoosting(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gamma_exp, 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0040, 0.9936, 1.0037], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "gamma_exp = torch.load('C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')\n",
    "print(gamma_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0214, Accuracy: 59612/60000 (99.353%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_model = torch.load('C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()\n",
    "\n",
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "#         if batch_idx == 937:\n",
    "#             print(\"Output before is:\",output)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data)\n",
    "#             if batch_idx == 937:\n",
    "#                 print(\"Hmodel output is:\", model(data))\n",
    "#                 print(\"Output after is:\",output)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)\n",
    "# test_losses.append(loss)    \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_models):\n",
    "    model = models[i]\n",
    "    torch.save(model, 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_models = 3\n",
    "models = []\n",
    "for x in range(num_of_models):\n",
    "    globals()['model%s' % x] = torch.load('C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(1) + '.pth')\n",
    "    models.append(globals()['model%s' % x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 83.42 %\n"
     ]
    }
   ],
   "source": [
    "initial_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW(initial_model)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attack_ensemble(object):\n",
    "    r\"\"\"\n",
    "    Base class for all attacks.\n",
    "    .. note::\n",
    "        It automatically set device to the device where given model is.\n",
    "        It temporarily changes the original model's training mode to `test`\n",
    "        by `.eval()` only during an attack process.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, model, models, gamma):\n",
    "        r\"\"\"\n",
    "        Initializes internal attack state.\n",
    "        Arguments:\n",
    "            name (str) : name of an attack.\n",
    "            model (torch.nn.Module): model to attack.\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack = name\n",
    "        self.model = model\n",
    "        self.models = models\n",
    "        self.gamma = gamma\n",
    "        self.model_name = str(model).split(\"(\")[0]\n",
    "\n",
    "        self.training = model.training\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        self._targeted = 1\n",
    "        self._attack_mode = 'original'\n",
    "        self._return_type = 'float'\n",
    "\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"\n",
    "        It defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_attack_mode(self, mode):\n",
    "        r\"\"\"\n",
    "        Set the attack mode.\n",
    "  \n",
    "        Arguments:\n",
    "            mode (str) : 'original' (DEFAULT)\n",
    "                         'targeted' - Use input labels as targeted labels.\n",
    "                         'least_likely' - Use least likely labels as targeted labels.\n",
    "        \"\"\"\n",
    "        if self._attack_mode is 'only_original':\n",
    "            raise ValueError(\"Changing attack mode is not supported in this attack method.\")\n",
    "            \n",
    "        if mode==\"original\":\n",
    "            self._attack_mode = \"original\"\n",
    "            self._targeted = 1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"targeted\":\n",
    "            self._attack_mode = \"targeted\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"least_likely\":\n",
    "            self._attack_mode = \"least_likely\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_least_likely_label\n",
    "        else:\n",
    "            raise ValueError(mode + \" is not a valid mode. [Options : original, targeted, least_likely]\")\n",
    "            \n",
    "    def set_return_type(self, type):\n",
    "        r\"\"\"\n",
    "        Set the return type of adversarial images: `int` or `float`.\n",
    "        Arguments:\n",
    "            type (str) : 'float' or 'int'. (DEFAULT : 'float')\n",
    "        \"\"\"\n",
    "        if type == 'float':\n",
    "            self._return_type = 'float'\n",
    "        elif type == 'int':\n",
    "            self._return_type = 'int'\n",
    "        else:\n",
    "            raise ValueError(type + \" is not a valid type. [Options : float, int]\")\n",
    "\n",
    "    def save(self, save_path, data_loader, verbose=True):\n",
    "        r\"\"\"\n",
    "        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.\n",
    "        Arguments:\n",
    "            save_path (str) : save_path.\n",
    "            data_loader (torch.utils.data.DataLoader) : data loader.\n",
    "            verbose (bool) : True for displaying detailed information. (DEFAULT : True)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        total_batch = len(data_loader)\n",
    "\n",
    "        for step, (images, labels) in enumerate(data_loader):\n",
    "            adv_images = self.__call__(images, labels)\n",
    "\n",
    "            image_list.append(adv_images.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "            if self._return_type == 'int':\n",
    "                adv_images = adv_images.float()/255\n",
    "\n",
    "            if verbose:\n",
    "                outputs = self.model(adv_images)\n",
    "                for i in range(len(self.models)):\n",
    "                    sub_model = self.models[i]\n",
    "                    outputs = outputs + self.gamma[i] * sub_model(adv_images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(self.device)).sum()\n",
    "\n",
    "                acc = 100 * float(correct) / total\n",
    "                print('- Save Progress : %2.2f %% / Accuracy : %2.2f %%' % ((step+1)/total_batch*100, acc), end='\\r')\n",
    "\n",
    "        x = torch.cat(image_list, 0)\n",
    "        y = torch.cat(label_list, 0)\n",
    "        torch.save((x, y), save_path)\n",
    "        print('\\n- Save Complete!')\n",
    "\n",
    "        self._switch_model()\n",
    "        \n",
    "    def _transform_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "        \n",
    "    def _get_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return input labels.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "    \n",
    "    def _get_least_likely_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return least likely labels.\n",
    "        \"\"\"\n",
    "        outputs = self.model(images)\n",
    "        for i in range(len(self.models)):\n",
    "            sub_model = self.models[i]\n",
    "            outputs = outputs + self.gamma[i] * sub_model(images)\n",
    "        _, labels = torch.min(outputs.data, 1)\n",
    "        labels = labels.detach_()\n",
    "        return labels\n",
    "    \n",
    "    def _to_uint(self, images):\n",
    "        r\"\"\"\n",
    "        Function for changing the return type.\n",
    "        Return images as int.\n",
    "        \"\"\"\n",
    "        return (images*255).type(torch.uint8)\n",
    "\n",
    "    def _switch_model(self):\n",
    "        r\"\"\"\n",
    "        Function for changing the training mode of the model.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.model.train()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].eval()\n",
    "\n",
    "    def __str__(self):\n",
    "        info = self.__dict__.copy()\n",
    "        \n",
    "        del_keys = ['model', 'attack']\n",
    "        \n",
    "        for key in info.keys():\n",
    "            if key[0] == \"_\" :\n",
    "                del_keys.append(key)\n",
    "                \n",
    "        for key in del_keys:\n",
    "            del info[key]\n",
    "        \n",
    "        info['attack_mode'] = self._attack_mode\n",
    "        if info['attack_mode'] == 'only_original' :\n",
    "            info['attack_mode'] = 'original'\n",
    "            \n",
    "        info['return_type'] = self._return_type\n",
    "        \n",
    "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
    "\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        self.model.eval()\n",
    "        for i in range(len(self.models)):\n",
    "            self.models[i].eval()\n",
    "        images = self.forward(*input, **kwargs)\n",
    "        self._switch_model()\n",
    "\n",
    "        if self._return_type == 'int':\n",
    "            images = self._to_uint(images)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "class CW_Ensemble(Attack_ensemble):\n",
    "    r\"\"\"\n",
    "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
    "    [https://arxiv.org/abs/1608.04644]\n",
    "    Distance Measure : L2\n",
    "        \n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        c (float): c in the paper. parameter for box-constraint. (DEFALUT : 1e-4)    \n",
    "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`    \n",
    "        kappa (float): kappa (also written as 'confidence') in the paper. (DEFALUT : 0)\n",
    "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
    "        steps (int): number of steps. (DEFALUT : 1000)\n",
    "        lr (float): learning rate of the Adam optimizer. (DEFALUT : 0.01)\n",
    "        \n",
    "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
    "    \n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "          \n",
    "    Examples::\n",
    "        >>> attack = torchattacks.CW(model, targeted=False, c=1e-4, kappa=0, steps=1000, lr=0.01)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "        \n",
    "    .. note:: NOT IMPLEMENTED methods in the paper due to time consuming.\n",
    "    \n",
    "        (1) Binary search for c.\n",
    "        \n",
    "        (2) Choosing best L2 adversaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, models, gamma, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super(CW_Ensemble, self).__init__(\"CW\", model, models, gamma)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        labels = self._transform_label(images, labels)\n",
    "\n",
    "        # f-function in the paper\n",
    "        def f(x):\n",
    "            outputs = self.model(x)\n",
    "            for i in range(len(self.models)):\n",
    "                sub_model = self.models[i]\n",
    "                outputs = outputs + self.gamma[i] * sub_model(x)\n",
    "            one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "            i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "            j = torch.masked_select(outputs, one_hot_labels.bool())\n",
    "\n",
    "            return torch.clamp(self._targeted*(j-i), min=-self.kappa)\n",
    "\n",
    "        w = torch.zeros_like(images).to(self.device)\n",
    "        w.detach_()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "        prev = 1e10\n",
    "\n",
    "        for step in range(self.steps):\n",
    "\n",
    "            a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "            loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "            loss2 = torch.sum(self.c*f(a))\n",
    "\n",
    "            cost = loss1 + loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stop when loss does not converge.\n",
    "            if step % (self.steps//10) == 0:\n",
    "                if cost > prev:\n",
    "                    warnings.warn(\"Early stopped because the loss is not converged.\")\n",
    "                    return (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "                prev = cost\n",
    "\n",
    "            # print('- CW Attack Progress : %2.2f %%        ' %((step+1)/self.steps*100), end='\\r')\n",
    "\n",
    "        adv_images = (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "\n",
    "        return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 83.39 %\n"
     ]
    }
   ],
   "source": [
    "initial_model.eval()\n",
    "for i in range(num_of_models):\n",
    "    models[i].eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW_Ensemble(model = initial_model,models = models,gamma = gamma_exp)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        for i in range(num_of_models):\n",
    "            sub_model = models[i]\n",
    "            outputs = outputs + gamma_exp[i] * sub_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_ensemble_attack(initial_model, models, images, labels, targeted=False, c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01) :\n",
    "\n",
    "    images = images.to(device)     \n",
    "    labels = labels.to(device)\n",
    "\n",
    "    \n",
    "    def f(x) :\n",
    "\n",
    "        outputs = initial_model(x)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                outputs = outputs.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            outputs = outputs + gamma_temp * model(data)\n",
    "            \n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(device)\n",
    "\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "        j = torch.masked_select(outputs, one_hot_labels.byte())\n",
    "        \n",
    "        # If targeted, optimize for making the other class most likely \n",
    "        if targeted :\n",
    "            return torch.clamp(i-j, min=-kappa)\n",
    "        \n",
    "        # If untargeted, optimize for making the other class most likely \n",
    "        else :\n",
    "            return torch.clamp(j-i, min=-kappa)\n",
    "    \n",
    "    w = torch.zeros_like(images, requires_grad=True).to(device)\n",
    "\n",
    "    optimizer = optim.Adam([w], lr=learning_rate)\n",
    "\n",
    "    prev = 1e10\n",
    "    \n",
    "    for step in range(max_iter) :\n",
    "\n",
    "        a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "        loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "        loss2 = torch.sum(c*f(a))\n",
    "\n",
    "        cost = loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Early Stop when loss does not converge.\n",
    "        if step % (max_iter//10) == 0 :\n",
    "            if cost > prev :\n",
    "                print('Attack Stopped due to CONVERGENCE....')\n",
    "                return a\n",
    "            prev = cost\n",
    "        \n",
    "        print('- Learning Progress : %2.2f %%        ' %((step+1)/max_iter*100), end='\\r')\n",
    "\n",
    "    attack_images = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "    return attack_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
