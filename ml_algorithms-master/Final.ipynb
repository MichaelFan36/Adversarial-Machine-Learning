{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(HNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "#         x = F.softmax(x, dim = 1)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NHNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_exp = []\n",
    "train_output = []\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "# test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = HNet()\n",
    "\n",
    "optimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    initial_model = initial_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    initial_model.train()\n",
    "#     exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = initial_model(data)\n",
    "#         print(output.shape)\n",
    "#         train_output.append(output)\n",
    "#         if batch_idx == 937:      \n",
    "#             train_output.append(output)\n",
    "#         print(\"before:\",batch_idx,output[0])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(optimizer.state_dict())\n",
    "#         gamma_exp.append(optimizer.state_dict()['exp_avg'])\n",
    "#         gamma_exp_sq.append(optimizer.state_dict()['exp_avg_sq'])\n",
    "#         print(gamma[''])\n",
    "#         print(\"after:\",output[0])\n",
    "#         train_output.append(output.data.max(1, keepdim=True))\n",
    "#         if batch_idx == 0:\n",
    "#             print(output.data.max(1, keepdim=True)[1].shape)\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        torch.save(initial_model.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/model.pth')\n",
    "        torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/optimizer.pth')\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "#             train_losses.append(loss.item())\n",
    "#             train_counter.append(\n",
    "#                 (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    initial_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "#             data, target = Variable(data, volatile=True), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = initial_model(data)\n",
    "        \n",
    "            loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    test_losses.append(loss)    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.542295\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.354034\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.265088\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.151298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9eaadc6aa077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6a68a807b499>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n\u001b[0;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/cozyn/Desktop/Research/results/model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/cozyn/Desktop/Research/results/optimizer.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 65\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/initial_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HNet(\n",
       "  (flatten): Flatten()\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model = torch.load('C:/Users/cozyn/Desktop/Research/results/initial_model.pth')\n",
    "initial_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseresidual(y, F):\n",
    "    return y - F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hoptimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "Hcriterion = nn.MSELoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Hcriterion = Hcriterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a3e5811652bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Htrain(Hmodel, epoch):\n",
    "    Hmodel.train()\n",
    "    for m in range(num_of_models):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            nb_digits = 10\n",
    "            target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "                target_onehot = target_onehot.cuda()\n",
    "            Hoptimizer.zero_grad()\n",
    "            output = initial_model(data)\n",
    "            for i in range(m):\n",
    "                model = models[i]\n",
    "                if torch.cuda.is_available():\n",
    "                    output = output.cuda()\n",
    "                    model = model.cuda()\n",
    "                output = output + gamma_exp[i] * model(data)\n",
    "            target = target.view(-1,1)\n",
    "            target_onehot.zero_()\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            residual = mseresidual(target_onehot, output)\n",
    "            houtput = Hmodel(data)\n",
    "            houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "            residual = residual.type(torch.cuda.FloatTensor)\n",
    "#             residual_list.append(residual)\n",
    "    #             print(\"residual is:\", residual)\n",
    "    #             print(\"predicted is:\", houtput)\n",
    "            loss = Hcriterion(houtput, residual)\n",
    "            loss.backward(retain_graph=True)\n",
    "            Hoptimizer.step()\n",
    "            if (batch_idx + 1)% 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoosting(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64)\n",
    "    models = []\n",
    "    residual_list = []\n",
    "    for m in range(M):\n",
    "        # Create new model for training residuals\n",
    "        Hmodel = NHNet()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "#         Htrain(Hmodel, 100)\n",
    "#         for i in range(5):\n",
    "#             print(i)\n",
    "        Hmodel.train()\n",
    "        epoch = 2\n",
    "        for i in range(epoch):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                Hoptimizer.zero_grad()\n",
    "                output = initial_model(data)\n",
    "                for i in range(m):\n",
    "                    model = models[i]\n",
    "                    if torch.cuda.is_available():\n",
    "                        output = output.cuda()\n",
    "                        model = model.cuda()\n",
    "                    output = output + gamma_exp[i] * model(data)\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                residual = mseresidual(target_onehot, output)\n",
    "                houtput = Hmodel(data)\n",
    "                houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                residual = residual.type(torch.cuda.FloatTensor)\n",
    "                residual_list.append(residual)\n",
    "    #             print(\"residual is:\", residual)\n",
    "    #             print(\"predicted is:\", houtput)\n",
    "                loss = Hcriterion(houtput, residual)\n",
    "                loss.backward(retain_graph=True)\n",
    "                Hoptimizer.step()\n",
    "                if (batch_idx + 1)% 100 == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                        100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "#             print('batch_idx', batch_idx)\n",
    "#             train_losses.append(loss.item())\n",
    "#             torch.save(model.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/model.pth')\n",
    "#             torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/optimizer.pth')\n",
    "        models.append(Hmodel)\n",
    "#         print(\"Hmodel is:\", Hmodel)\n",
    "#         print(\"Appended model is:\", models[1])\n",
    "#         print(\"Length of models:\", len(models))\n",
    "        \n",
    "        \n",
    "#         loss = 0\n",
    "#         correct = 0\n",
    "    \n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, (data, target) in enumerate(train_loader):\n",
    "# #             data, target = Variable(data, volatile=True), Variable(target)\n",
    "#                 residual = residual_list[batch_idx]\n",
    "#                 if torch.cuda.is_available():\n",
    "#                     data = data.cuda()\n",
    "#                     residual = residual.cuda()\n",
    "#                 residual = residual.type(torch.cuda.LongTensor)\n",
    "#                 output = Hmodel(data)\n",
    "#                 loss += F.cross_entropy(output, residual, reduction='sum').item()\n",
    "#                 pred = output.data.max(1, keepdim=True)[1]\n",
    "#                 correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "#         loss /= len(residual_list)\n",
    "#     # test_losses.append(loss)    \n",
    "#         print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "#             loss, correct, len(residual_list),\n",
    "#             100. * correct / len(residual_list)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "#         gamma[0] = 0.1\n",
    "        print(\"Initialized gamma:\", gamma)\n",
    "#         Variable(gamma)\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.003)\n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                for i in range(m):\n",
    "                    model = models[i]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[i]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                temp = Hmodel(data)\n",
    "    #             print('output is:', output)\n",
    "    #             print('gamma is:', gamma.shape)\n",
    "    #             print('Hmodel(data) is:', temp)\n",
    "                predicted = output + gamma * temp\n",
    "    #             print(\"predicted is:\", predicted)\n",
    "    #             predicted.double()\n",
    "    #             target_onehot.double()\n",
    "                loss = Hcriterion(predicted, target_onehot)\n",
    "    #             print(\"target_onehot is:\", target_onehot)\n",
    "    #             train_losses.append(loss.item())\n",
    "    #             torch.save(model.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/model.pth')\n",
    "    #             torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/results/optimizer.pth')\n",
    "    #             print(\"loss is:\", loss)\n",
    "                loss.backward(retain_graph=True)\n",
    "#                 print(\"gamma is before:\", gamma)\n",
    "    #             print(\"gamma's gradient is:\", gamma.retain_grad())\n",
    "                Goptimizer.step()\n",
    "#                 print(\"gamma is after:\", gamma)\n",
    "        gamma_exp[m] = gamma\n",
    "        print(gamma_exp)\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 3.506784\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.554577\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.339617\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.103660\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.497568\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.387566\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.345623\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.255483\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.241077\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.176665\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.158851\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.244647\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.112221\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.107801\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.113899\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.095893\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.491723\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.093556\n",
      "Initialized gamma: tensor([0.3014], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.096659\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.114558\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.102914\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.091001\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090524\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090153\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090271\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.089828\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090378\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.089979\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090247\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090310\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.089916\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090244\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090252\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.089979\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090154\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090321\n",
      "Initialized gamma: tensor([0.7089], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0067, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.092389\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090795\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090284\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.090168\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090424\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090190\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090645\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090629\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090099\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.090251\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.089986\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090329\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.090725\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090155\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.089687\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090061\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090544\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090462\n",
      "Initialized gamma: tensor([0.0156], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0067, 0.9911, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.090600\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090691\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.089542\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.089991\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090379\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090346\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090046\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090191\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089953\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.089955\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090131\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.089857\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.089978\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090401\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090322\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.089840\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090005\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089940\n",
      "Initialized gamma: tensor([0.5003], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0067, 0.9911, 0.9985, 1.0000, 1.0000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.090326\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.089785\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090314\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.090236\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.089737\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090803\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090073\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.089909\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090089\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.090743\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090476\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090588\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.090424\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.089655\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090531\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.090383\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090648\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090314\n",
      "Initialized gamma: tensor([0.1176], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0067, 0.9911, 0.9985, 0.9955, 1.0000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.090087\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090314\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.089756\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.089775\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090225\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090315\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.089895\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.089689\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090298\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.089894\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090194\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.090095\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.090626\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090282\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.089901\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.089802\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090320\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090050\n",
      "Initialized gamma: tensor([0.7884], device='cuda:0', requires_grad=True)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([1.0943, 1.0067, 0.9911, 0.9985, 0.9955, 1.0006], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "num_of_models = 6\n",
    "models, gamma_exp = GradientBoosting(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0325, Accuracy: 59504/60000 (99.173%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "#         if batch_idx == 937:\n",
    "#             print(\"Output before is:\",output)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data) * 20\n",
    "#             if batch_idx == 937:\n",
    "#                 print(\"Hmodel output is:\", model(data))\n",
    "#                 print(\"Output after is:\",output)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)\n",
    "# test_losses.append(loss)    \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = models[0]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model1.pth')\n",
    "model2 = models[1]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model2.pth')\n",
    "model3 = models[2]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model3.pth')\n",
    "model4 = models[3]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model4.pth')\n",
    "model5 = models[4]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model5.pth')\n",
    "model6 = models[5]\n",
    "torch.save(initial_model, 'C:/Users/cozyn/Desktop/Research/results/model6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
